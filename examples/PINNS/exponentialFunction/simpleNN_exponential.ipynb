{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to physics informed neural networks (PINNS)\n",
    "In the previous examples we have trained neural networks to represent function by feeding it with a set of observations at certain input points $x, y(x)$. and we have minimized the mean squared error between observations and predictions:\n",
    "This in turn may require many training samples, particularly in the case of complex functions. In the case one has prior information regarding the physical behaviour of $y$ such information could be used in combination with observations when training a neural network. In this setting recent developments in [automatic differentiation](https://arxiv.org/abs/1502.05767) can be exploited, which allow to differentiate neural networks with respect to their input coordinates to obtain [physics informed neural networks](https://arxiv.org/abs/1711.10561) (PINNS). In this setting let us consider a general nonlinear partial differential equation of the general form\n",
    "\n",
    "\\begin{equation}\n",
    "u_t +  \\Lambda \\left[u;\\lambda \\right] = 0\\,,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Lambda$ is a differential operator, and $\\lambda$ are parameters. In such a case are obtained training a neural network, such that it minimize discenpencies between observations $u\\left(x_i, t^n \\right)$ and predictions, while also satisfying (or at least minimize deviation from) the left hand side of it's govering nequation:\n",
    "\\begin{equation}\n",
    "f := u_t +  \\Lambda \\left[ u \\right] \\,,\n",
    "\\end{equation}\n",
    "\n",
    "In order to obtain a loss function\n",
    "\\begin{equation}\n",
    "loss  = \\mathrm{MSE}_u + \\mathrm{MSE}_f = \\frac{1}{N_u}\\sum_{i=1}^{N_u}\\left(u\\left(t_u^i, x_u^i\\right) - u_i\\right)^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}\\left(f\\left(t_f^i, x_f^i\\right)\\right)^2  \\,,\n",
    "\\end{equation}\n",
    "\n",
    "i.e. by evaluting the loss function based on evaluating the neural nets performance for predicting $u$, a set of $\\left\\{t_u^i, x_u^i, u^i\\right\\}_{i=1}^{N_u}$ training points (initial and boundary data) and by additionally evaluating the left hand side of the governing equation, $f$ on a set of points $\\left\\{t_f^i, x_f^i\\right\\}_{i=1}^{N_f}$.\n",
    "\n",
    "## Solve ODEs by minimizing a combination of observations and physical (equations) \n",
    "\n",
    "In order to introuduce PINNS we'll start by training a neural net to represent the exponential function $y\\left(x\\right) = e^x$. We'll start by learning to represent the function from a set of (training) points, y. We'll then see how we can train the same network by constraining it to satisfy (or at least minimize deviation from) it's governing equation \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d \\,y\\left(x\\right)}{dx}  - y\\left(x\\right) = 0  \\,.\n",
    "\\end{equation}\n",
    "\n",
    " In this case we have only one independent variable, $x$, and the loss function for the regual neural network, and for the PINN simplify to:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{loss}_\\mathrm{NN} =  \\mathrm{MSE}_y  = \\frac{1}{N_y}\\sum_{i=1}^{N_y}\\left(u\\left(x_y^i\\right) - y_i\\right)^2  \\,,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{loss}_\\mathrm{PINN}  = \\mathrm{MSE}_u + \\mathrm{MSE}_f = \\frac{1}{N_{y, PINN}}\\sum_{i=1}^{N_{y, PINN}}\\left(u\\left(x_y^i\\right) - y_i\\right)^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}\\left(f\\left(x_f^i\\right)\\right)^2  \\,,\n",
    "\\end{equation}\n",
    "\n",
    "where $f = y_x - y$. Note that the number of observations $N_y$ and $N_{y, PINN}$ does not have to be the same in $\\mathrm{loss}_\\mathrm{NN}$ and $\\mathrm{loss}_\\mathrm{PINN}$, since in general we would like to use PINNS in cases where we have sparse amount of data/observations. In fact (in this example) we will only provide the PINN with one observed $y$ value. The number of points $N_f$ where $f$ is evaluated, one the other hand, is not constrained by lack of data, and we are in principle free to chose as many points as we want. But in n this example we'll compare the regular NN  and PINN in cases where $N_{y, PINN} + N_f \\approx N_y$.\n",
    "\n",
    "In both cases we'll use a single hidden layer, like the one below, however the number of neurons can be changed.\n",
    "\n",
    "\n",
    "<img src=\"fig/neuralNet.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problems\n",
    "- compare accuracy (MSE) using the regular NN and PINN\n",
    "- compare accuracy (MSE) using regular NN and PINN in the case of extrapolation, i.e. when training is done on a limited set of data, and predictions are obtained outside this area.\n",
    "- modify the code to represent y(x) = cos(x), in which f = y + d^2y/dx^2\n",
    "- modify the code to represent y(x) = e^x * cos(x), in which f: y - dy/y_x + 0.5*d^2y/dx^2\n",
    "- In the latter two cases, try to increase the domain bounds. Also try with and without feeding the NN with scaled (normalized between 0 and 1) inputs. For the PINN you may need to feed it with a few more observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#==========================================================================================#\n",
    "# Set parampeters (hyperameters) and compute a training data set for the desired function  #\n",
    "#==========================================================================================#\n",
    "\n",
    "N = 10001 # Number of training points\n",
    "N_train = 10 # Number of points to train on per training attempt\n",
    "N_neurons = 5 # Number of neurons in the hidden layer\n",
    "afunc = tf.nn.sigmoid # Set activation function, other functions .relu, .sigmoid, .tanh, .elu\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 5000 # Number of epochs\n",
    "\n",
    "\n",
    "#Set x-domain boundaries\n",
    "x_start = 0.\n",
    "x_end = 2.\n",
    "\n",
    "#ub = np.array([[x_end]]) #upper bound\n",
    "x_data = np.linspace(x_start, x_end, N)\n",
    "#===================================================\n",
    "# Specify what function you wish to train for\n",
    "#===================================================\n",
    "y_data = np.exp(x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "y_data = y_data[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "\n",
    "constrainUpper = 1 # set smaller than one to test extrapolation\n",
    "idx = np.random.choice(int(x_data.shape[0]*constrainUpper), N_train, replace=False)\n",
    "x_data_train = x_data[idx]\n",
    "y_data_train = y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set up placeholder for inputs and outputs\n",
    "#===================================================\n",
    "x = tf.placeholder(tf.float32, shape=[None, x_data.shape[1]], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, y_data.shape[1]], name='y')\n",
    "#===================================================\n",
    "# declare weights and biases input --> hidden layer\n",
    "#===================================================\n",
    "W1 = tf.Variable(tf.random_normal([1, N_neurons], stddev=0.5), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([N_neurons]), name='b1')\n",
    "#===================================================\n",
    "# declare weights and biases of hidden --> output layer\n",
    "#===================================================\n",
    "W2 = tf.Variable(tf.random_normal([N_neurons, 1], stddev=0.5), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='b2')\n",
    "#===================================================\n",
    "# declare output of NN\n",
    "#===================================================\n",
    "a1 = afunc(tf.add(tf.matmul(x, W1), b1)) # activation of hidden layer\n",
    "y_NN = tf.add(tf.matmul(a1, W2), b2) # computational graph for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot y_pred before training\n",
    "#===================================================\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    y_pred_init = sess.run(y_NN, feed_dict = {x:x_data})\n",
    "plt.figure()\n",
    "plt.plot(x_data.flatten(), y_data.flatten())\n",
    "plt.plot(x_data_train.flatten(), y_data_train.flatten(), 'o')\n",
    "plt.plot(x_data.flatten(), y_pred_init.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train the model using regular NN\n",
    "#===================================================\n",
    "loss = tf.reduce_mean(tf.square(y - y_NN))\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        _, c = sess.run([optimiser, loss], \n",
    "                     feed_dict={x: x_data_train, y: y_data_train})\n",
    "        avg_cost += c\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost))\n",
    "\n",
    "    y_pred = sess.run(y_NN, feed_dict = {x:x_data, y:y_data})\n",
    "    loss_pred = sess.run(loss, feed_dict = {x:x_data, y:y_data})\n",
    "    plt.figure()\n",
    "    plt.plot(x_data.flatten(), y_data.flatten(), 'o')\n",
    "    plt.plot(x_data.flatten(), y_pred.flatten(),'r--')\n",
    "    plt.plot(x_data_train.flatten(), y_data_train.flatten(), 'bo')\n",
    "    plt.legend([\"y\", \"y_NN\", \"y_train\"])\n",
    "    print(\"MSE_u on all data (N: {}): \".format(x_data.shape[0]), loss_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot y_pred before training\n",
    "#===================================================\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    y_pred_init = sess.run(y_NN, feed_dict = {x:x_data})\n",
    "plt.figure()\n",
    "plt.plot(x_data.flatten(), y_data.flatten())\n",
    "plt.plot(x_data_train.flatten(), y_data_train.flatten(), 'o')\n",
    "\n",
    "plt.plot(x_data.flatten(), y_pred_init.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train the model using PINNS\n",
    "#===================================================\n",
    "\n",
    "x_f = tf.placeholder(tf.float32, shape=[None, x_data.shape[1]], name='x_f')\n",
    "a1_PINNS = afunc(tf.add(tf.matmul(x_f, W1), b1))\n",
    "y_NN_PINNS = tf.add(tf.matmul(a1_PINNS, W2), b2)\n",
    "# note that dy_dx_NN, y_NN_PINNS and y_NN share the same weights and biases\n",
    "dy_dx_NN = tf.gradients(y_NN_PINNS, x_f)[0]\n",
    "\n",
    "\n",
    "f =  dy_dx_NN - y_NN_PINNS\n",
    "\n",
    "MSE_u = tf.reduce_mean(tf.square(y - y_NN))\n",
    "MSE_f = tf.reduce_mean(tf.square(f))\n",
    "loss_PINNS = MSE_u + MSE_f\n",
    "optimiser_PINNS = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss_PINNS)\n",
    "\n",
    "# we only use one training point to minimize MSEu\n",
    "x_data_train_PINNS = x_data_train[0:1,0:1] \n",
    "y_data_train_PINNS = y_data_train[0:1,0:1]\n",
    "\n",
    "\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        _, c = sess.run([optimiser_PINNS, loss_PINNS ], \n",
    "                     feed_dict={x: x_data_train_PINNS, y: y_data_train_PINNS, x_f: x_data_train})\n",
    "        avg_cost += c\n",
    "        MSE_u_value, MSE_f_value = sess.run([MSE_u, MSE_f], \n",
    "                     feed_dict={x: x_data_train_PINNS, y: y_data_train_PINNS, x_f: x_data_train})\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost), \"MSE_u =\", \"{:.6f}\".format(MSE_u_value), \"MSE_f =\", \"{:.6f}\".format(MSE_f_value))\n",
    "\n",
    "    y_pred = sess.run(y_NN, feed_dict = {x:x_data, y:y_data})\n",
    "    loss_pred = sess.run(loss, feed_dict = {x:x_data, y:y_data, x_f:x_data})\n",
    "    plt.figure()\n",
    "    plt.plot(x_data.flatten(), y_data.flatten(), 'o')\n",
    "    plt.plot(x_data.flatten(), y_pred.flatten(),'r--')\n",
    "    plt.plot(x_data_train.flatten(), y_data_train.flatten(), 'bo')\n",
    "    plt.plot(x_data_train_PINNS.flatten(), y_data_train_PINNS.flatten(), 'go')\n",
    "    plt.legend([\"y\", \"y_NN\", \"y_train(x_f)\", \"y_train\"])\n",
    "    print(\"MSE_u on all data (N: {}): \".format(x_data.shape[0]), loss_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sympy cells to calculate f for some simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x_sp = sp.Symbol('x')\n",
    "y_sp = sp.cos(x_sp)\n",
    "dy_sp_dx = sp.diff(y_sp, x_sp)\n",
    "dy_sp_dx2 = sp.diff(dy_sp_dx, x_sp)\n",
    "#y_sp, dy_sp_dx, dy_sp_dx2\n",
    "y_sp + dy_sp_dx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sp = sp.Symbol('x')\n",
    "y_sp = sp.cos(x_sp)*sp.exp(x_sp)\n",
    "dy_sp_dx = sp.diff(y_sp, x_sp)\n",
    "dy_sp_dx2 = sp.diff(dy_sp_dx, x_sp)\n",
    "#y_sp, dy_sp_dx, dy_sp_dx2\n",
    "y_sp - dy_sp_dx + 0.5*dy_sp_dx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
