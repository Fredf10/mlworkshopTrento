{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNS applied on the steady state 1D equation\n",
    "\n",
    "In this notebook we will try to solve the steady state 1D momentum equation for blood flow in rigid domains. \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x } \\left(\\frac{Q^2}{A}\\right) = -\\frac{A}{\\rho}\\frac{\\partial P}{\\partial x} - \\frac{8 \\, \\mu \\, \\pi \\, Q}{\\rho \\, A}\\,,\n",
    "\\end{equation}\n",
    "which may be reformulated to\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial P}{\\partial x}  = -\\frac{\\rho}{A} \\frac{\\partial}{\\partial x } \\left(\\frac{Q^2}{A}\\right)- \\frac{8 \\, \\mu \\, \\pi \\, Q}{\\, A^2}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "In this case we'll treat Q as given in which and we may express P(x) as:\n",
    "\n",
    "\\begin{equation}\n",
    "P\\left(x\\right)  = \\int^x -\\frac{\\rho}{A} \\frac{\\partial}{\\partial x } \\left(\\frac{Q^2}{A}\\right) dx  + \\int^x- \\frac{8 \\, \\mu \\, \\pi \\, Q}{\\, A^2} dx\\,,\n",
    "\\end{equation}\n",
    "\n",
    "Our f function is defined as \n",
    "\\begin{equation}\n",
    "f  = \\frac{\\partial P}{\\partial x} - I_c - I_f \\,,\n",
    "\\end{equation}\n",
    "where $I_c=-\\frac{\\rho}{A} \\frac{\\partial}{\\partial x } \\left(\\frac{Q^2}{A}\\right)$ and $I_f=- \\frac{8 \\, \\mu \\, \\pi \\, Q}{\\, A^2}$, in which $I_c$ and $I_f$ are source terms.\n",
    "\n",
    "## Code and functions to calculate solution and source terms for different geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def sinusGeom_np(x, R0, Rmin, l):\n",
    "    r = R0 + ((R0 - Rmin)/2)*(np.cos(2*np.pi*x/l) - 1)\n",
    "    \n",
    "    return r\n",
    "\n",
    "def sinusGeom_sp(x, R0, Rmin, l):\n",
    "    r = R0 + ((R0 - Rmin)/2)*(sp.cos(2*sp.pi*x/l) - 1)\n",
    "    \n",
    "    return r\n",
    "\n",
    "def linearTapering(x, R0, Rmin, l):\n",
    "    \n",
    "    r = R0 + (Rmin - R0)*x\n",
    "    \n",
    "    return r\n",
    "\n",
    "def integrand_friction_sp(r, mu, Q):\n",
    "    \n",
    "    A = sp.pi*r**2\n",
    "    \n",
    "    I_f_sp = -8*mu*sp.pi*Q/(A**2)\n",
    "    \n",
    "    return I_f_sp\n",
    "\n",
    "def integrand_convective_sp(r, rho, Q):\n",
    "    \n",
    "    A = sp.pi*r**2\n",
    "    \n",
    "    I_c_sp = -rho*sp.diff(Q**2/A)/A\n",
    "    \n",
    "    return I_c_sp\n",
    "\n",
    "def calcDeltaP(x, I_f, I_c, diffusive=True, convective=True, P_in=0):\n",
    "    \n",
    "    P = np.zeros(len(x))\n",
    "    P[0] = P_in\n",
    "    P_f = np.zeros(len(x))\n",
    "    P_f[0] = P_in\n",
    "    P_c = np.zeros(len(x))\n",
    "    P_c[0] = P_in\n",
    "    I_f_array = np.zeros(len(x))\n",
    "    I_c_array = np.zeros(len(x))\n",
    "    if diffusive:\n",
    "        I_f_array[0] = I_f(x[0])\n",
    "    if convective:\n",
    "        I_c_array[0] = I_c(x[0])\n",
    "    for n in range(len(x) - 1):        \n",
    "        \n",
    "        dp = 0\n",
    "        dp_f = quad(I_f, x[n], x[n + 1])[0]\n",
    "        dp_c = quad(I_c, x[n], x[n + 1])[0]\n",
    "        if diffusive:\n",
    "            dp += dp_f\n",
    "            P_f[n + 1] = P_f[n] + dp_f\n",
    "            I_f_array[n + 1] = I_f(x[n + 1])\n",
    "        if convective:\n",
    "            dp += dp_c\n",
    "            P_c[n + 1] = P_c[n] + dp_c\n",
    "            I_c_array[n + 1] = I_c(x[n + 1])\n",
    "        P[n + 1] = P[n] + dp\n",
    "    \n",
    "    return P, P_f, P_c, I_f_array, I_c_array\n",
    "\n",
    "def get1D_data(N, geometryType=\"sine\", showPlots=True, diffusive=True, convective=True):\n",
    "\n",
    "    R0 = 0.2    # [cm]\n",
    "    Rmin = 0.05 # [cm]\n",
    "    l = 1       # [cm]\n",
    "    Q = 2       # [ml/s]\n",
    "    rho = 1.05  # [g/cm^3]\n",
    "    mu = 0.035  # [P] (g/(cm s))\n",
    "    #N = 1001\n",
    "    x_np = np.linspace(0, l, N)\n",
    "    \n",
    "    if geometryType == \"sine\":\n",
    "        geomFunc_np = sinusGeom_np\n",
    "        geomFunc_sp = sinusGeom_sp\n",
    "    elif geometryType == \"linearTapering\":\n",
    "        geomFunc_np = linearTapering\n",
    "        geomFunc_sp = linearTapering\n",
    "    elif geometryType == \"constant\":\n",
    "        geomFunc_np = linearTapering\n",
    "        geomFunc_sp = linearTapering\n",
    "        R0 = Rmin\n",
    "    r_np = geomFunc_np(x_np, R0, Rmin, l)\n",
    "\n",
    "    x = sp.Symbol('x')\n",
    "    r_sp = geomFunc_sp(x, R0, Rmin, l)#R0 + ((R0 - Rmin)/2)*(sp.cos(2*sp.pi*x/l) - 1)\n",
    "\n",
    "    integrand_f = integrand_friction_sp(r_sp, mu, Q)\n",
    "    integrand_c = integrand_convective_sp(r_sp, rho, Q)\n",
    "\n",
    "    integrand_f = sp.lambdify([x], integrand_f)\n",
    "    integrand_c = sp.lambdify([x], integrand_c)\n",
    "\n",
    "    P_np, P_f_np, P_c_np, integrand_f_array, integrand_c_array = calcDeltaP(x_np, integrand_f, integrand_c,\n",
    "                                                                           diffusive=diffusive, convective=convective)\n",
    "\n",
    "    #showPlots = False\n",
    "    if showPlots:\n",
    "        plt.figure()\n",
    "        plt.plot(x_np, r_np)\n",
    "        #plt.plot(x_np, linearTapering_np(x_np, R0, Rmin, l))\n",
    "        plt.xlabel(\"x [cm]\")\n",
    "        plt.ylabel(\"r [cm]\")\n",
    "        plt.figure()\n",
    "        plt.plot(x_np, P_np/1333.2)\n",
    "        plt.plot(x_np, P_f_np/1333.2)\n",
    "        plt.plot(x_np, P_c_np/1333.2)\n",
    "        plt.legend([\"P\", \"P_f\", \"P_c\"])\n",
    "        #plt.plot(x_np, linearTapering_np(x_np, R0, Rmin, l))\n",
    "        plt.xlabel(\"x [cm]\")\n",
    "        plt.ylabel(\"P [mmHg]\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(x_np, integrand_f_array/1333.2)\n",
    "        plt.plot(x_np, integrand_c_array/1333.2)\n",
    "        plt.legend([\"I_f (dP_dx_f)\", \"I_c (dP_dx_c)\"])\n",
    "        #plt.plot(x_np, linearTapering_np(x_np, R0, Rmin, l))\n",
    "        plt.xlabel(\"x [cm]\")\n",
    "        plt.ylabel(\"dP_dx [mmHg/cm]\")\n",
    "    \n",
    "    return x_np, r_np, P_np, P_f_np, P_c_np, integrand_f_array, integrand_c_array\n",
    "\n",
    "#x_np, r_np, P_np, P_f_np, P_c_np, integrand_f_array, integrand_c_array = get1D_data(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and functions do define neural net, f-function and initialization of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_u(x):\n",
    "    u = neural_net(x, weights, biases)\n",
    "    return u\n",
    "#===================================================\n",
    "\n",
    "def net_f(x, I_f, I_c):\n",
    "    \n",
    "    u = net_u(x)\n",
    "    \n",
    "    u_x = tf.gradients(u, x)\n",
    "    \n",
    "    f =  u_x - I_f - I_c\n",
    "    \n",
    "    return f\n",
    "#===================================================\n",
    "\n",
    "def initialize_NN(layers):        \n",
    "    weights = []\n",
    "    biases = []\n",
    "    num_layers = len(layers) \n",
    "    for l in range(0, num_layers - 1):\n",
    "        W = xavier_init(size=[layers[l], layers[l+1]])\n",
    "        #W = tf.Variable(tf.random_normal([layers[l], layers[l+1]], stddev=10))\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        weights.append(W)\n",
    "        biases.append(b)        \n",
    "    return weights, biases\n",
    "#===================================================\n",
    "  \n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]        \n",
    "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "#===================================================\n",
    "\n",
    "def neural_net(X, weights, biases):\n",
    "    num_layers = len(weights) + 1\n",
    "    \n",
    "    H = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    for l in range(0, num_layers - 2):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "    W = weights[-1]\n",
    "    b = biases[-1]\n",
    "    Y = tf.add(tf.matmul(H, W), b)\n",
    "    return Y\n",
    "#===================================================\n",
    "\n",
    "def callback(loss):\n",
    "    print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#===================================================\n",
    "# load data\n",
    "#===================================================\n",
    "N = 1001 # total number of x, P values\n",
    "N_train = 10 # number of training points\n",
    "N_train_f = 20 # number of training points\n",
    "geometryType = \"constant\" # [\"sine\", \"linearTapering\", \"constant\"]\n",
    "layers = [1, 5, 1]\n",
    "diffusive = True\n",
    "convective = True\n",
    "showPlots = True\n",
    "x, r, P, P_f, P_c, I_f, I_c = get1D_data(N, geometryType=geometryType, showPlots=showPlots,\n",
    "                                                         diffusive=diffusive, convective=convective)\n",
    "dyneTommHg = 1./1333.22368\n",
    "\n",
    "scaleInputs = True\n",
    "if scaleInputs:\n",
    "    P *= dyneTommHg\n",
    "    P_f *= dyneTommHg\n",
    "    P_c *= dyneTommHg\n",
    "    I_f *= dyneTommHg\n",
    "    I_c *= dyneTommHg\n",
    "    \n",
    "#===================================================\n",
    "# turn 1D array into 2D array of shape (N, 1)\n",
    "#===================================================\n",
    "x = x[:, np.newaxis]\n",
    "P = P[:, np.newaxis]\n",
    "I_f = I_f[:, np.newaxis]\n",
    "I_c = I_c[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set parameters for neural network and training\n",
    "#===================================================\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(np.shape(I_f), np.shape(I_c))\n",
    "#===================================================\n",
    "# sample random points for training\n",
    "#===================================================\n",
    "idx = np.random.choice(x.shape[0], N_train, replace=False)\n",
    "idx_f = np.random.choice(x.shape[0], N_train_f, replace=False)\n",
    "x_train = x[idx]\n",
    "P_train = P[idx]\n",
    "#x_train = x[0:1, 0:1]\n",
    "#P_train = P[0:1, 0:1]\n",
    "\n",
    "x_train_f = x[idx_f]\n",
    "I_f_train_f = np.interp(x_train_f.flatten(), x.flatten(), I_f.flatten())\n",
    "I_f_train_f = I_f_train_f[:, np.newaxis]\n",
    "I_c_train_f = np.interp(x_train_f.flatten(), x.flatten(), I_c.flatten())\n",
    "I_c_train_f = I_c_train_f[:, np.newaxis]\n",
    "print(np.shape((I_c_train_f)))\n",
    "#===================================================\n",
    "# set up placeholder for inputs and outputs\n",
    "#===================================================\n",
    "x_tf = tf.placeholder(tf.float32, shape=[None, x.shape[1]], name='x')\n",
    "P_tf = tf.placeholder(tf.float32, shape=[None, P.shape[1]], name='P')\n",
    "\n",
    "x_f_tf = tf.placeholder(tf.float32, shape=[None, x.shape[1]], name='x')\n",
    "I_f_tf = tf.placeholder(tf.float32, shape=[None, I_f.shape[1]], name='integrand_f')\n",
    "I_c_tf = tf.placeholder(tf.float32, shape=[None, I_c.shape[1]], name='integrand_c')\n",
    "#===================================================\n",
    "# initialize neural net and f functions\n",
    "#===================================================\n",
    "lb = x.min(0)\n",
    "ub = x.max(0)\n",
    "weights, biases = initialize_NN(layers)\n",
    "\n",
    "P_pred = net_u(x_tf)\n",
    "f_pred = net_f(x_f_tf, I_f_tf, I_c_tf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot y_pred before training\n",
    "#===================================================\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    P_pred_init = sess.run(P_pred, feed_dict = {x_tf:x_train})\n",
    "plt.figure()\n",
    "plt.plot(x.flatten(), P.flatten())\n",
    "plt.plot(x_train.flatten(), P_train.flatten(), 'o')\n",
    "plt.plot(x_train.flatten(), P_pred_init.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train the model using PINNS and GradientDescentOptimizer\n",
    "#===================================================\n",
    "\n",
    "MSE_u = tf.reduce_mean(tf.square((P_tf - P_pred)))\n",
    "MSE_f = tf.reduce_mean(tf.square(f_pred))/100\n",
    "loss = MSE_u + MSE_f\n",
    "learning_rate_value = 0.01\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "learning_rate_value_late = 0.01#25\n",
    "epochs = 10000\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        if epoch < 2000:\n",
    "            learning_rate_value_epoch = learning_rate_value\n",
    "        else:\n",
    "            learning_rate_value_epoch = learning_rate_value_late\n",
    "        _, c, MSE_u_value, MSE_f_value = sess.run([optimiser, loss, MSE_u, MSE_f], \n",
    "                     feed_dict={x_tf: x_train, P_tf: P_train, \n",
    "                                x_f_tf: x_train_f, I_f_tf: I_f_train_f,\n",
    "                                I_c_tf: I_c_train_f, learning_rate: learning_rate_value_epoch})\n",
    "        \n",
    "        \n",
    "        avg_cost += c\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost), \"MSE_u =\", \"{:.6f}\".format(MSE_u_value), \"MSE_f =\", \"{:.6f}\".format(MSE_f_value))\n",
    "\n",
    "    P_result = sess.run(P_pred, feed_dict = {x_tf:x})\n",
    "    P_result_f = sess.run(P_pred, feed_dict = {x_tf: x_train_f})\n",
    "    plt.figure()\n",
    "    plt.plot(x.flatten(), P.flatten())\n",
    "    plt.plot(x.flatten(), P_result.flatten(), '--')\n",
    "    plt.plot(x_train.flatten(), P_train.flatten(), 'o')\n",
    "    plt.plot(x_train_f.flatten(), P_result_f.flatten(), 'o')\n",
    "    plt.xlabel(\"x [cm]\")\n",
    "    plt.ylabel(\"P [mmHg]\")\n",
    "    plt.legend([\"P(x)\", \"P_PINN\", \"P_train\", \"P_train_f\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train the model using regular NN and GradientDescentOptimizer\n",
    "#===================================================\n",
    "\n",
    "loss_NN = MSE_u\n",
    "learning_rate_value = 0.01\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "learning_rate_value_late = 0.01#25\n",
    "epochs = 10000\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        if epoch < 2000:\n",
    "            learning_rate_value_epoch = learning_rate_value\n",
    "        else:\n",
    "            learning_rate_value_epoch = learning_rate_value_late\n",
    "        _, c, MSE_u_value, MSE_f_value = sess.run([optimiser, loss_NN, MSE_u, MSE_f], \n",
    "                                 feed_dict={x_tf: x_train, P_tf: P_train, \n",
    "                                            x_f_tf: x_train_f, I_f_tf: I_f_train_f,\n",
    "                                            I_c_tf: I_c_train_f, learning_rate: learning_rate_value_epoch})\n",
    "        \n",
    "        \n",
    "        avg_cost += c\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost), \"MSE_u =\", \"{:.6f}\".format(MSE_u_value), \"MSE_f =\", \"{:.6f}\".format(MSE_f_value))\n",
    "\n",
    "    P_result = sess.run(P_pred, feed_dict = {x_tf:x})\n",
    "    P_result_f = sess.run(P_pred, feed_dict = {x_tf: x_train_f})\n",
    "    plt.figure()\n",
    "    plt.plot(x.flatten(), P.flatten())\n",
    "    plt.plot(x.flatten(), P_result.flatten(), '--')\n",
    "    plt.plot(x_train.flatten(), P_train.flatten(), 'o')\n",
    "    plt.plot(x_train_f.flatten(), P_result_f.flatten(), 'o')\n",
    "    plt.legend([\"P(x)\", \"P_PINN\", \"P_train\", \"P_train_f\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative opimizer PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, \n",
    "                                                   method = 'L-BFGS-B', \n",
    "                                                   options = {'maxiter': 500,\n",
    "                                                              'maxfun': 50000,\n",
    "                                                              'maxcor': 50,\n",
    "                                                              'maxls': 50,\n",
    "                                                              'ftol' : 1.0 * np.finfo(float).eps})\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tf_dict = {x_tf: x_train, P_tf: P_train, \n",
    "               x_f_tf: x_train_f, I_f_tf: I_f_train_f,\n",
    "               I_c_tf: I_c_train_f}\n",
    "    optimizer.minimize(sess, \n",
    "                       feed_dict = tf_dict,         \n",
    "                       fetches = [loss], \n",
    "                       loss_callback = callback)\n",
    "\n",
    "    P_result = sess.run(P_pred, feed_dict = {x_tf: x_train_f})\n",
    "    P_result_all = sess.run(P_pred, feed_dict = {x_tf: x})\n",
    "    plt.figure()\n",
    "    plt.plot(x.flatten(), P.flatten())\n",
    "    plt.plot(x.flatten(), P_result_all.flatten(), '--')\n",
    "    plt.plot(x_train.flatten(), P_train.flatten(), 'o')\n",
    "    plt.plot(x_train_f.flatten(), P_result.flatten(), 'o')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative opimizer regular NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_NN = tf.reduce_mean(tf.square((P_tf - P_pred)))\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss_NN, \n",
    "                                                   method = 'L-BFGS-B', \n",
    "                                                   options = {'maxiter': 500,\n",
    "                                                              'maxfun': 50000,\n",
    "                                                              'maxcor': 50,\n",
    "                                                              'maxls': 50,\n",
    "                                                              'ftol' : 1.0 * np.finfo(float).eps})\n",
    "\n",
    "x_train_NN = x[idx_f]\n",
    "P_train_NN = P[idx_f]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tf_dict = {x_tf: x_train_NN, P_tf: P_train_NN}\n",
    "    optimizer.minimize(sess, \n",
    "                       feed_dict = tf_dict,         \n",
    "                       fetches = [loss_NN], \n",
    "                       loss_callback = callback)\n",
    "\n",
    "    P_result = sess.run(P_pred, feed_dict = {x_tf: x_train_NN})\n",
    "    P_result_all = sess.run(P_pred, feed_dict = {x_tf: x})\n",
    "    plt.figure()\n",
    "    plt.plot(x.flatten(), P.flatten())\n",
    "    plt.plot(x.flatten(), P_result_all.flatten(), '--')\n",
    "    plt.plot(x_train_NN.flatten(), P_train_NN.flatten(), 'o')\n",
    "    plt.plot(x_train_NN.flatten(), P_result.flatten(), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
