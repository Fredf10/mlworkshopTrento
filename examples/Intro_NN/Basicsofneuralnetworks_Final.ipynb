{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductions to neural networks\n",
    "## Perceptrons and logic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron, see figure below is a binary classifier and can take any number of inputs. For a first example we shall consider a single perceptron and how this represents and can train a boolean function.\n",
    "\n",
    "<img src=\"percept_img/PerceptronSymbol.png\" width=\"400\">\n",
    "\n",
    "In theory a perceptron can have any number of inputs but for our case we use only two, $x_1 \\in \\{0, 1\\}$ and $x_2 \\in \\{0, 1\\}$ to represent the simplest boolean founctions with output $y \\in \\{0,1\\}$.\n",
    "\n",
    "Each perceptron input $x_1$ $x_2$ has an associated weight $w_1$ and $w_2$, and the output $y$ is determined when the weighted sum $\\sum_i w_i x_i$ is entered in an activation function given as follows\n",
    "\n",
    "\\begin{equation}\\label{eq:}\n",
    "y = \n",
    "\\begin{cases}\n",
    " 1 & \\mathrm{if} \\sum_i w_i x_i \\geq \\theta_1 \\\\  \n",
    " 0 & \\mathrm{if} \\sum_i w_i x_i < \\theta_1 \\\\ \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\text heta_1$ is the activation threshold or bias for the activation function, and hence this is effectively a step function. Due to the small inputspace we only have 4 combinations of inputs. We want to se if we can train this node to become a boolean AND function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic boolean functions AND/OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "- Manually tune the parameters of the single perceptron to create a logic AND function and then a logic OR.\n",
    "- Can you output the same if you keep the bias $\\theta$ at 0?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def activation_function(x,w,theta):\n",
    "    wtsum = np.dot(x,w)\n",
    "    output = 0\n",
    "    if (wtsum >= theta): output=1\n",
    "    return output\n",
    "\n",
    "%matplotlib inline\n",
    "def boolean_matrix(w1,w2,theta):\n",
    "    #Print matrix\n",
    "    #os.system('clear')\n",
    "    print('+-----+-----+-----+')\n",
    "    print('+ x_1 + x_2 +  y  +')\n",
    "    print('+-----+-----+-----+')\n",
    "    print('+  0  +  0  +  '+str(activation_function([0,0],[w1,w2],theta))+'  +')\n",
    "    print('+-----+-----+-----+')\n",
    "    print('+  1  +  0  +  '+str(activation_function([1,0],[w1,w2],theta))+'  +')\n",
    "    print('+-----+-----+-----+')\n",
    "    print('+  0  +  1  +  '+str(activation_function([0,1],[w1,w2],theta))+'  +')\n",
    "    print('+-----+-----+-----+')\n",
    "    print('+  1  +  1  +  '+str(activation_function([1,1],[w1,w2],theta))+'  +')\n",
    "    print('+-----+-----+-----+')\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    plt.plot([0],[0],'ok')\n",
    "    plt.plot([1],[0],'ok')\n",
    "    plt.plot([0],[1],'ok')\n",
    "    plt.plot([1],[1],'ok')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x_1')\n",
    "    plt.ylabel('x_2')\n",
    "    plt.xlim(-0.5,1.5)\n",
    "    plt.ylim(-0.5,1.5)\n",
    "    plt.title('Hyperplane')\n",
    "    if(w1 == 0):\n",
    "        x1=np.linspace(-0.1*theta/1.e-2,1.1*theta/1.e-5,51)\n",
    "    else:\n",
    "        x1=np.linspace(-0.1*theta/w1,1.1*theta/w1,51)\n",
    "    if(w2 == 0):\n",
    "        plt.plot(x1,(theta-w1*x1)/1.e-2,'k')\n",
    "    else:\n",
    "        plt.plot(x1,(theta-w1*x1)/w2,'k') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interactive(boolean_matrix,\n",
    "            w1=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=0.0, continuous_update=False),\n",
    "            w2=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=0.0, continuous_update=False),\n",
    "            theta=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=1.0, continuous_update=False)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bias zero we struggle to define our functions, which also makes sense considering the hyperplane separating value 1 and value 0 coordinates, must now go though the origin. Let us now try to train the perceptron using \"the perceptron rule\".\n",
    "\n",
    "The change in weights is expressed as the rule\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta w_i = \\eta (y-\\hat{y}) x_i,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\eta$ is the training rate, $y$ is the output as determined by training data, while $\\hat{y}$ is the estimated output.\n",
    "\n",
    "Let us give the training point: activation_function(x_1 = 1,x_2 = 1) = 1, and se if we manage to get an AND function from this data point alone if we start with weights equal to 2, and a bias $\\theta = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_single(y,x1,x2,w1,w2,iterations):\n",
    "    #Assuming y, and x can be lists of coordinates and outputs\n",
    "    for i in range(0,iterations):\n",
    "        try:\n",
    "            for ind,element in enumerate(y):\n",
    "                dw = deltaw(y[ind],[x1[ind],x2[ind]],[w1,w2])\n",
    "                w1 += dw[0]\n",
    "                w2 += dw[1]\n",
    "        except:\n",
    "            dw = deltaw(y,[x1,x2],[w1,w2])\n",
    "            w1 += dw[0]\n",
    "            w2 += dw[1]\n",
    "    return w1,w2\n",
    "\n",
    "def deltaw(y,x,w):\n",
    "    trate = 0.05\n",
    "    return trate*(y - activation_function(x,w,0.5))*x[0],trate*(y - activation_function(x,w,0.5))*x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial state\")\n",
    "boolean_matrix(0.0,0.0,0.5)\n",
    "\n",
    "#Specify training data\n",
    "y = [1,2]# ,0,0,0]\n",
    "x1 = [0,1]#,1,0,0]\n",
    "x2 = [1,0]#,0,1,0]\n",
    "\n",
    "#Initial weights\n",
    "w1 = 0.0\n",
    "w2 = 0.0\n",
    "\n",
    "w1, w2 = training_single(y,x1,x2,w1,w2,iterations=100)\n",
    "print('Weights (w1,w2):',w1, w2)\n",
    "boolean_matrix(w1,w2,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try setting a new set of points and train an OR function with the fewest set of points possible. Try to change the number of iterations between 1 and 100. The training rate $\\eta$ is set to 0.05.\n",
    "\n",
    "You cannot make an exclusive or (XOR) function however, as this requires either two hyperplanes or a curved one. Let us see if we can create one by adding another node and create a network, for we know we can create any boolean function by combining AND, OR and NOT functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"percept_img/PerceptronXOR.png\" width=\"400\">\n",
    " \n",
    "Keeping the parameters for the first node to that of an AND function (for example: $w_1=0.5, w_2=0.5, \\theta=0.5$) and adding a second perceptron, we can create the XOR. Use the silders under to tune the XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def boolean_matrix_XOR(w1,w2,w3,w4,w5,theta1,theta2):\n",
    "    #Print matrix\n",
    "    #os.system('clear')\n",
    "    w0 = [w1,w2]\n",
    "    w = [w3,w5,w4]\n",
    "    xset = np.zeros([4,2])\n",
    "    xset[:,0] = [0,1,0,1]\n",
    "    xset[:,1] = [0,0,1,1]\n",
    "    \n",
    "    print('+-----+-----+-----+-----+')\n",
    "    print('+ x_1 + x_2 + y_1 + y_2 +')\n",
    "    for i in range(0,len(xset[:,0])):\n",
    "        xpoint = xset[i,:]\n",
    "        y1 = activation_function(xpoint,w0,theta1)\n",
    "        xpoint = np.append(xpoint,y1)\n",
    "        #print(xpoint,w)\n",
    "        print('+-----+-----+-----+-----+')\n",
    "        print('+  '+str(int(xpoint[0]))+'  +  '+str(int(xpoint[1]))+'  +  '+str(y1)+'  +  '+str(activation_function(xpoint,w,theta2))+'  +')\n",
    "        print('+-----+-----+-----+-----+')\n",
    "    \n",
    "    x1=np.linspace(-0.5,1.5,51)\n",
    "    x2=x1\n",
    "    \n",
    "    xx1, xx2 = np.meshgrid(x1,x2)\n",
    "\n",
    "    # calculate corresponding y\n",
    "    if(w4 == 0):\n",
    "        y = (theta2-xx1*w3-xx2*w5)/1.e-2\n",
    "    else:\n",
    "        y = (theta2-xx1*w3-xx2*w5)/w4\n",
    "    \n",
    "    # plot the surface\n",
    "    #plt3d = plt.figure().gca(projection='3d')\n",
    "    #plt3d.plot_surface(xx1, xx2, y, alpha=0.2)\n",
    "    \n",
    "    #plt3d.set_xlabel('x_1')\n",
    "    #plt3d.set_ylabel('x_2')\n",
    "    #plt3d.set_ylabel('y_2')\n",
    "    #plt3d.set_xlim(-0.5,1.5)\n",
    "    #plt3d.set_ylim(-0.5,1.5)\n",
    "    #plt3d.set_zlim(-0.5,1.5)\n",
    "    #plt3d.set_title('Hyperplane')\n",
    "    #ax = plt3d.gca()\n",
    "    #ax.hold(True)\n",
    "    #plt3d.scatter(xset[:,0], xset[:,1], [0,0,0,0], color='black')\n",
    "    #plt3d.scatter(xset[:,0], xset[:,1], [1,1,1,1], color='black')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interactive(boolean_matrix_XOR,\n",
    "            w1=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=0.0, continuous_update=False),\n",
    "            w2=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=0.0, continuous_update=False),\n",
    "            w3=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=0.0, continuous_update=False),\n",
    "            w4=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=0.0, continuous_update=False),\n",
    "            w5=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=0.0, continuous_update=False),\n",
    "            theta1=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=1.0, continuous_update=False),\n",
    "            theta2=widgets.FloatSlider(min=-2.0,max=2.0,step=0.05,value=1.0, continuous_update=False)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected feed forward neural networks\n",
    "\n",
    "<img src=\"percept_img/NeuralNet2.png\" width=\"800\">\n",
    "\n",
    "The previous examples show simple constructions of neural networks. In this section we'll look at fully connected feed forward neural networks, with multiple layers, and show how one can represent them in terms of a series of matrix multiplications. In this context \n",
    "- \"feed forward\" means that information is propagated in one direction only (from input to output) \n",
    "- \"fully connected\" means that all neurons in a layer $l$ is connected to all neurons in the previous ($l - 1$) and next ($l + 1$) layer.\n",
    "- also of note is that there is no direct passage of information other than via neighbouring layers. (i.e. the structure of the XOR neural net is not possible, where the input has direct links to hidden layer 1 and 2).\n",
    "\n",
    "The figure above shows a neural net with 1 input neuron, 2 hidden layers with 3 and 2 neurons respectively, and 1 output layer with 1 neron. In the figure we have also depicted the input and output (activations), all weights connecting the neurons, in addition to biases and activation of each neuron, which follows the following naming conventions:\n",
    "\n",
    "- $\\omega^l_{j,k}$ is the weight from the $k$<sup>th</sup> neuron in the ($l - 1$)<sup>th</sup> layer to the $j$<sup>th</sup> neuron in the $l$<sup>th</sup> layer\n",
    "- $b^l_{j}$ is the bias of the $j$<sup>th</sup> neuron in the $l$<sup>th</sup> layer\n",
    "- $a^l_{j}$ is the activation of the $j$<sup>th</sup> neuron in the $l$<sup>th</sup> layer\n",
    "\n",
    "With this naming convention we note that the activation of neuron $a_j^l$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "a_j^l = \\sigma \\left( \\sum_k \\omega_{j, k}^l a_k^{l - 1} + b_j^l\\right) \\,,\n",
    "\\end{equation}\n",
    "where $\\sigma$ is an activation function. In the case of using perceptron neurons the output of $a_j^l$ would be: \n",
    "\\begin{equation}\\label{eq:}\n",
    "\\mathrm{output} = \n",
    "\\begin{cases}\n",
    " 1 & \\mathrm{if} \\left( \\sum_k \\omega_{j, k}^l a_k^{l - 1} + b_j^l\\right) \\geq 0 \\\\  \n",
    " 0 & \\mathrm{if} \\left( \\sum_k \\omega_{j, k}^l a_k^{l - 1} + b_j^l\\right) < 0 \n",
    "\\end{cases} \\,,\n",
    "\\end{equation}\n",
    "but we note that $\\sigma$ can be any function. In any case one can see that the activation of a layer $l$, $a^l$ can be represented in a vectorized form:\n",
    "\\begin{equation}\n",
    "a^l = \\sigma \\left( \\omega^l a^{l - 1} + b^l\\right) \\,,\n",
    "\\end{equation}\n",
    "\n",
    "With this the activation of the different layers in the example above can be computed as:\n",
    "\\begin{equation}\n",
    "a^1 = \\sigma \\left(  \\left[\\begin{matrix}w^1_{11}\\\\w^1_{21}\\\\w^1_{31}\\end{matrix}\\right] \\cdot \\left[\\begin{matrix}x\\end{matrix}\\right] + \\left[\\begin{matrix}b^1_{1}\\\\b^1_{2}\\\\b^1_{3}\\end{matrix}\\right]\\right) \\,, \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "a^2 = \\sigma \\left( \\left[\\begin{matrix}w^2_{11} & w^2_{12} & w^2_{13}\\\\w^2_{21} & w^2_{22} & w^2_{23}\\end{matrix}\\right] \\cdot \\left[\\begin{matrix}a^1_{1}\\\\a^1_{2}\\\\a^1_{3}\\end{matrix}\\right]  + \\left[\\begin{matrix}b^2_{1}\\\\b^2_{2}\\end{matrix}\\right]\\right) \n",
    "\\,,\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "y = \\sigma \\left( \\left[\\begin{matrix}w^3_{13} & w^3_{23}\\end{matrix}\\right] \\cdot \\left[\\begin{matrix}a^2_{1}\\\\a^2_{2}\\end{matrix}\\right] + \\left[\\begin{matrix}b^3_{1}\\end{matrix}\\right] \\right)\n",
    "\\,,\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up neural networks in tensorflow\n",
    "TensorFlow is an open source platform for machine learning, which we will use in this notebook. Unless otherwise enabled (through twnsorflow.executing_eagerly()) tensorflow computations are run using symbolic handles through <em>graphs</em>. Execution of a graph is performed in tensorflow sessions. \n",
    "- Quoted from the TensorFlow website; \"A computational graph (or graph in short) is a series of TensorFlow operations arranged into a graph of nodes\". Basically, it means a graph is just an arrangement of nodes that represent the operations in your model.\n",
    "\n",
    "Before we set up the neural net above, let's look at a simple tensorflow graph, and run it in a session\n",
    "## TensorFlow graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.set_random_seed(1)\n",
    "a_scalar = 2\n",
    "b_scalar = 3\n",
    "c_scalar = tf.add(a_scalar, b_scalar, name='Add')\n",
    "print(c_scalar)\n",
    "\n",
    "W_tmp = tf.constant([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "a_tmp = tf.constant([[1], [2], [3]])\n",
    "b_tmp = tf.constant([[1], [2], [3]])\n",
    "a2_tmp = tf.add(tf.matmul(W_tmp, a_tmp), b_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the first graph in a session\n",
    "sess = tf.Session()\n",
    "print(sess.run(c_scalar))\n",
    "sess.close()\n",
    "\n",
    "# run the second graph in a session\n",
    "# using 'with tf.Session() as sess:' in which we do not need to close the session\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a2_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow variables and placeholders\n",
    "Trainable parameters such as weights and biases are declared using 'tensorflow.variable', whereas placeholders are used to feed actual training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([3, 1], stddev=0.75), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([3, 1]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([2, 3], stddev=0.75), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([2, 1]), name='b2')\n",
    "W3 = tf.Variable(tf.random_normal([1, 2], stddev=0.75), name='W3')\n",
    "b3 = tf.Variable(tf.random_normal([1, 1]), name='b3')\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[1, 1], name='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can declare the weights and biases and create graphs for the different layers in the above example neural network. For simplicity we'll assume a linear activation function, $\\sigma(x)=x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = tf.add(tf.matmul(W1, x), b1)\n",
    "a2 = tf.add(tf.matmul(W2, a1), b2)\n",
    "y = tf.add(tf.matmul(W3, a2), b3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An finally we can initialize weights and biases and evaluate the neural network by feeding it with a (training example) x value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[1]])\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    print(sess.run(y, feed_dict = {x:x_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we have treated vectors as column-vectors, however by default tensorflow assumes row-vectors, and we will thus in the following work with the transpose the equation for the activation of a layer:\n",
    "\\begin{equation}\n",
    "(a^l)^T = \\sigma \\left( \\left(\\omega^l a^{l - 1}\\right)^T + \\left(b^l\\right)^T\\right) = \\sigma \\left( {a^{l - 1}}^T {\\omega^l}^T  + \\left(b^l\\right)^T\\right)\\,,\n",
    "\\end{equation}\n",
    "\n",
    "With this convention the above neural network can be implemented as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([1, 3], stddev=0.75), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([3]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([3, 2], stddev=0.75), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([2]), name='b2')\n",
    "W3 = tf.Variable(tf.random_normal([2, 1], stddev=0.75), name='W3')\n",
    "b3 = tf.Variable(tf.random_normal([1]), name='b3')\n",
    "x = tf.placeholder(tf.float32, shape=[1, 1], name='x')\n",
    "a1 = tf.add(tf.matmul(x, W1), b1)\n",
    "a2 = tf.add(tf.matmul(a1, W2), b2)\n",
    "y = tf.add(tf.matmul(a2, W3), b3)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    print(sess.run(y, feed_dict = {x:x_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do neural networks represent functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the perceptron rule generalises poorly to multiple layers of perceptrons and was an initial hurdle for the further development of neural networks. However, if we start introducting differentiable activation functions for every node we can rather work with the gradient descent rule, which is formalised as following:\n",
    "\n",
    "If we define an error function as the sum of squared deviations from the data points $y$ we can write it as\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{2}\\sum_j ( y_j - \\hat{y_j} )^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $j$ spans all points in a set of training data and\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y_j} = \\sum_j w_j x_j - \\theta\n",
    "\\end{equation}\n",
    "\n",
    "if we then differentiate with respect to a weight $w_i$\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial w_i} = \\sum_j ( y_j - \\hat{y_j}) (-\\sum_j x_j \\frac{\\partial w_j}{\\partial w_i}) = \\sum_j ( y_j - \\hat{y_j}) (-\\sum_j x_j \\delta_{ij}) = - \\sum_j ( y_j - \\hat{y_j}) x_i\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows for backpropagation which is a technique for minimizing the error throughout a multilayered network. And we won't go through the seams of how this works, but we will use it in the coming examples. \"The universal approximation theorem\" states that:\n",
    "\n",
    "### Theorem\n",
    "- \"A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.\"\n",
    "\n",
    "\n",
    "So let us see how a simple neural network with a single hidden layer may be trained to represent a polynomial function. We will also learn to know the tensorflow package in python.\n",
    "\n",
    "Some of the most normal activation functions are listed below. The parametric Rectified Linear Unit is often reffered to as the \"leaky\" ReLU, and the logistic function is also known as the sigmoid.\n",
    "\n",
    "<img src=\"percept_img/ActivationFunctions.png\" width=\"800\">\n",
    "<center>Source: https://towardsdatascience.com</center>\n",
    "\n",
    "We will now try to represent a few functions using a single layer network, with a varying number of nodes, and activation functions.\n",
    "\n",
    "We shall also mention that in the example below we employ a technique in ensemble learning where we pick randomly sampled subsets from the training data, for which we learn from each sample and average the functions we learn from each sample called an \"epoch\"\n",
    "\n",
    "### Problems\n",
    "Attempt to represent a: \n",
    "\n",
    "- linear function $f(x) = 2x+3$ using ReLU activation functions with 5, 10 and 20 nodes\n",
    "\n",
    "- polynomial function $f(x) = x^3 + x^2 + x - 1$ using ReLU activation functions, sigmoid and tanh with 5, 10 and 100 nodes for $x \\in [-2,2]$, learning rate = 0.02 and 10000 epochs. Try a higher learning rate for the sigmoid $\\approx$ 0.1\n",
    "\n",
    "- exponential function $f(x) = e^x$ using the \"leaky ReLU\", tanh and exponential linear unit with ..5 nodes\n",
    "\n",
    "If you are interested, try to add some normally distributed white noise of standard deviation $\\pm 2\\%$ to the training data set. You may have to tune the hyperameters, learning rate, number of training points and epochs to make this work. Remember to check that the cost function values should diminish per epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "#==========================================================================================#\n",
    "# Set parampeters (hyperameters) and compute a training data set for the desired function  #\n",
    "#==========================================================================================#\n",
    "\n",
    "N = 10001 # Number of training points\n",
    "N_train = 100 # Number of points to train on per training attempt\n",
    "N_neurons = 10#10 # Number of neurons in the hidden layer\n",
    "afunc = tf.nn.elu # Set activation function, other functions .relu, .sigmoid, .tanh, .elu\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000 # Number of subsamples to learn from  per epoch\n",
    "\n",
    "\n",
    "#Set x-domain boundaries\n",
    "x_start = -2.\n",
    "x_end = 2.\n",
    "x_data = np.linspace(x_start, x_end, N)\n",
    "#===================================================\n",
    "# Specify what function you wish to train for\n",
    "#===================================================\n",
    "#y_data = 2.0*x_data + 3.0 #Linear polynomial function\n",
    "#y_data = x_data**3 + x_data**2 + x_data - 1.0 #Cubic polynomial function\n",
    "y_data = np.exp(x_data) #Exponential function\n",
    "\n",
    "#y_data = (1 + 0.02*np.random.randn(len(y_data)))*y_data #Added noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "y_data = y_data[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "\n",
    "idx = np.random.choice(x_data.shape[0], N_train, replace=False)\n",
    "x_data_train = x_data[idx]\n",
    "y_data_train = y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set up placeholder for inputs and outputs\n",
    "#===================================================\n",
    "x = tf.placeholder(tf.float32, shape=[None, x_data.shape[1]], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, y_data.shape[1]], name='x')\n",
    "#===================================================\n",
    "# declare weights and biases input --> hidden layer\n",
    "#===================================================\n",
    "W1 = tf.Variable(tf.random_normal([1, N_neurons], stddev=0.75), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([N_neurons]), name='b1')\n",
    "#===================================================\n",
    "# declare weights and biases of hidden --> output layer\n",
    "#===================================================\n",
    "W2 = tf.Variable(tf.random_normal([N_neurons, 1], stddev=0.75), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='b2')\n",
    "#===================================================\n",
    "# declare output of NN\n",
    "#===================================================\n",
    "hidden_out = afunc(tf.add(tf.matmul(x, W1), b1)) #Apply activation function for inputs to hidden layer\n",
    "y_NN = tf.add(tf.matmul(hidden_out, W2), b2) #Apply linear sum for outputs from each hidden layer node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================\n",
    "# plot y_pred before training using only initial conditions\n",
    "#===========================================================\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    y_pred_init = sess.run(y_NN, feed_dict = {x:x_data})\n",
    "plt.figure()\n",
    "plt.plot(x_data.flatten(), y_data.flatten())\n",
    "plt.plot(x_data.flatten(), y_pred_init.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# Train the model\n",
    "#===================================================\n",
    "batch_size = N_train\n",
    "loss = tf.reduce_mean(tf.square(y - y_NN)) # Minimize the mean (least) square error\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        _, c = sess.run([optimiser, loss], \n",
    "                     feed_dict={x: x_data_train, y: y_data_train})\n",
    "        avg_cost += c\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost))\n",
    "\n",
    "    y_pred = sess.run(y_NN, feed_dict = {x:x_data, y:y_data})\n",
    "    loss_pred = sess.run(loss, feed_dict = {x:x_data, y:y_data})\n",
    "    plt.figure()\n",
    "    plt.plot(x_data.flatten(), y_data.flatten())\n",
    "    plt.plot(x_data.flatten(), y_pred.flatten())\n",
    "    print(loss_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
