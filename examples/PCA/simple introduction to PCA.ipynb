{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple introduction tutorial on PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimesionality reduction\n",
    "\n",
    "The theme of this tutorial is the concept of dimensionality reduction, which  refers to the idea of representing the information present in set of features with dimensions $N$ by a set of features with dimension $M \\ll N$.\n",
    "\n",
    "First we consider an observation of all the features for a single case which we denote as $\\mathbf{x}^{(i)}$ where \n",
    "\\begin{equation}\n",
    "\\mathbf{x} = \\begin{bmatrix} \\text{feature 1} & \\text{feature 2} & \\cdots &\\text{feature N}\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "We would like to determine a mapping\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\approx \\mathbf{A} \\mathbf{t}\n",
    "\\end{equation}\n",
    "where $\\mathbf{t} \\in \\mathbb{R}^{M}$\n",
    "\n",
    "Thinking of the values of $\\mathbf{t}$ as corrdinates in a lower dimensional space we have reduced the dimensionality of the representation of the information in $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A *very* simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(2,100)\n",
    "cov_mat = np.array([[2,0.5],[1,0.5]])\n",
    "data_trans = np.dot(cov_mat, data)\n",
    "plt.figure(\"2D-example\")\n",
    "#plt.plot(data[0],data[1],'o')\n",
    "plt.plot(data_trans[0],data_trans[1],'o')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitive understanding\n",
    "* Looking at this figure what do you notice? \n",
    "* How would you describe this to someone if they couldn't see the picture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"2D-example\")\n",
    "plt.quiver(0, 0, cov_mat[0,0], cov_mat[1,0],  angles='xy', scale_units='xy', scale=1, zorder=1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The idea of PCA \n",
    "\n",
    "PCA formalizes this intuitive idea by identifying directions (and thus subspaces) which preserve the most variation when the data vectors are projected onto the subspace.\n",
    "\n",
    "# Random vectors and the covariance matrix\n",
    "First, we consider our feature vectors as being generated by some probability distribution\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\sim \\mathcal{D}(\\mu, \\Sigma)\n",
    "\\end{equation}\n",
    "where $\\mu$ is the expected value $E(\\mathbf{x}) = \\mu$ and $\\Sigma$ is the covariance matrix.\n",
    "\n",
    "The covariance matrix may be defined (for $\\mu=0$ WLOG) as\n",
    "\\begin{equation}\n",
    "\\Sigma = E(\\mathbf{x}\\mathbf{x}^\\top)\n",
    "\\end{equation}\n",
    "(here we treat a single realization as a column vector).\n",
    "\n",
    "Now we assemble our data matrix by recording each observation of a case (denoted $\\mathbf{x}^{(i)}$) as a row:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}^{(1)} \\\\\n",
    "                             \\vdots \\\\\n",
    "                             \\mathbf{x}^{(N_s)} \\\\\n",
    "                             \\end{bmatrix}\n",
    "\\end{equation}\n",
    "(implicitly here we treate observations as row vectors)\n",
    "\n",
    "Thus the sample covariance matrix is \n",
    "\\begin{equation}\n",
    "\\hat{\\Sigma} = \\mathbf{X}^\\top \\mathbf{X}\n",
    "\\end{equation}\n",
    "Note that $ \\hat{\\Sigma}_{ij} = \\sum X_{ki} X_{kj} = \\sum \\mathbf{x}^{(k)}_i \\mathbf{x}^{(k)}_j$ where $i$ and $j$ denote factors and $k$ indicates a particular sample.\n",
    "\n",
    "# Linear transformations and covariance\n",
    "We may consider a linear transformation of a random vector $\\mathbf{c}^\\top \\mathbf{x}$. What is it's covariance matrix\n",
    "\\begin{equation}\n",
    "\\operatorname{cov}(\\mathbf{c}^\\top \\mathbf{x}) = E(\\mathbf{c}^\\top \\mathbf{x}\\mathbf{x}^\\top\\mathbf{c}) = \\mathbf{c}^\\top E(\\mathbf{x}\\mathbf{x}^\\top) \\mathbf{c} = \\mathbf{c}^\\top \\Sigma \\mathbf{c}.  \n",
    "\\end{equation}\n",
    "\n",
    "*Note* that if $\\mathbf{c} $ is a vector, $\\mathbf{c}^\\top \\mathbf{x}$ is a scalar, so the covariance is just the variance.\n",
    "\n",
    "\n",
    "# So, is there a way to formulate dimensionality reduction based on this concept?\n",
    "\n",
    "If we restrict our consideration to unit vectors $c$, we might seek to find a $c$ which maximizes \n",
    "$\\mathbf{c}^\\top \\Sigma \\mathbf{c}$ , i.e. which single dimensional variable depending linearly on $\\mathbf{x}$ has maximum variance?\n",
    "\n",
    "Replacing $\\Sigma$ with $\\hat{\\Sigma}$ results in maximizing\n",
    "\\begin{equation}\n",
    " \\mathbf{c}^\\top \\mathbf{X}^\\top \\mathbf{X} \\mathbf{c} = \\lVert \\mathbf{X} \\mathbf{c} \\rVert^2\n",
    " \\end{equation}\n",
    "which is the Rayleigh quotient and attains a maximum of the largest eigenvalue of $\\mathbf{X}^\\top \\mathbf{X}$ when $\\mathbf{c}$ is the corresponding eigenvector.\n",
    "\n",
    "We can show that the subspace spanned by the first k eigenvectors is the k-dimensional subspace that perserves the greatest variability of the data. (Due to the fact that eigenvectors are orthogonal for these matrices).\n",
    "\n",
    "This leads us to consider the Eigendecomposition of the sample covariance matrix\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^\\top \\mathbf{X} = \\mathbf{V} \\Lambda \\mathbf{V}^\\top\n",
    "\\end{equation}\n",
    "where $\\mathbf{V}$ is an orthogonal matrix and $\\Lambda$ is a diagonal matrix of eigenvalues.\n",
    "\n",
    "This is closely connected to the singular value decomposition of $\\mathbf{X}$\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{U} \\mathbf{S} \\mathbf{V}^\\top\n",
    "\\end{equation}\n",
    "Note that then \n",
    "\\begin{equation}\n",
    "\\mathbf{X}^\\top \\mathbf{X} =  \\mathbf{V} \\mathbf{S} \\mathbf{U}^\\top  \\mathbf{U} \\mathbf{S} \\mathbf{V}^\\top \n",
    "\\end{equation}\n",
    "since $\\mathbf{U}$ is unitary/orthogonal $\\mathbf{U}^\\top  \\mathbf{U} = \\mathbf{I}$ and\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^\\top \\mathbf{X} =  \\mathbf{V} \\mathbf{S}^2 \\mathbf{V}^\\top \n",
    "\\end{equation}\n",
    "so the eigenvalues of the sample covariance are the squares of the singular values of $\\mathbf{X}$.\n",
    "\n",
    "# Approximating $\\mathbf{X}$ with principal components\n",
    "This leads us to a view of how principal components are useful for approximating $\\mathbf{X}$\n",
    "we can denote $\\mathbf{T} = \\mathbf{U} \\mathbf{S}$\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{T} \\mathbf{V}^\\top.\n",
    "\\end{equation}\n",
    "$\\mathbf{T}$ is the score matrix it consists of the magnitude of the projection of each row of $\\mathbf{X}$ onto the eigenvectors which are the columns of $\\mathbf{V}$; explicitly \n",
    "\\begin{equation}\n",
    "T_{ij} = \\mathbf{x}^{(i)} \\cdot \\mathbf{v}_j.\n",
    "\\end{equation}\n",
    "\n",
    "We then can form an approximation of $\\mathbf{X}$ by including only the first k principal components. \n",
    "\n",
    "## How do we modify the matrices $\\mathbf{T}$ and $\\mathbf{V}$ that we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested solution follows\n",
    "\n",
    "Please\n",
    "\n",
    "Try\n",
    "\n",
    "This\n",
    "\n",
    "At\n",
    "\n",
    "Home\n",
    "\n",
    "before \n",
    "\n",
    "scrolling\n",
    "\n",
    "further.\n",
    "\n",
    "[There's a monster at the end of this notebook](https://www.goodreads.com/book/show/44186.The_Monster_at_the_End_of_this_Book). You don't want to get to the monster do you?\n",
    "![Image of monster](https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1388193494l/44186.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we modify the matrices $\\mathbf{T}$ and $\\mathbf{V}$ that we have?\n",
    "\n",
    "Truncate to $k$ principal components by using only the first $k$ columns of $\\mathbf{T}$ and $\\mathbf{V}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} \\approx \\mathbf{T}_k \\mathbf{V}_k^\\top = \\sum_{i=0}^k \\mathbf{t}_i \\mathbf{v}_i^\\top = \\sum_{i=0}^k \\mathbf{t}_i \\otimes \\mathbf{v}_i\n",
    "\\end{equation}\n",
    "\n",
    "so we can see that given the values or scores of the principal components and the vectors defining their directions we can reconstruct $\\mathbf{X}$ or even a single observation $\\mathbf{x}^{(i)}$ from the score matrix $\\mathbf{T}$ or just the scores\n",
    "\\begin{equation}\n",
    "t_j = \\mathbf{x}^{(i)} \\cdot \\mathbf{v}_j\n",
    "\\end{equation}\n",
    "\n",
    "So given new data items, we could also make use of the matrix $\\mathbf{V}$ to determine the represenation of $\\mathbf{x}^{(i)}$ according the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_algorithm_evd(X):\n",
    "    \"\"\"\n",
    "    An algorithm to find the principal components by Eigendecomposition of X' * X\n",
    "    Args:\n",
    "        X (ndarray): A n_s (number of samples) by n_d (number of features/dimensions)\n",
    "            matrix where each row is a sample and each column corresponds to features,\n",
    "            where the columns have zero mean.\n",
    "    Returns:\n",
    "        V (ndarray): A n_d by n_d matrix where the columns represents the ordered \n",
    "            directions that preserve maximal variance of the data.\n",
    "        ev_sorted (ndarray): a n_d vector of the squared eigenvalues of X' * X such \n",
    "            that the k-th value is proportional to the amount of variance preserved\n",
    "            when the data is projected onto the subspace spanned by V[:,k]\n",
    "        scores (ndarray): a n_s by n_d matrix where the rows are the values of the \n",
    "            samples in the coordinate system defined by V.\n",
    "    \"\"\"\n",
    "    covX = X.T @ X/(X.shape[0]-1)\n",
    "    ev, v = np.linalg.eig(covX)\n",
    "    ev_sort_idx = np.argsort(-np.abs(ev))\n",
    "    ev_sorted = ev[ev_sort_idx]\n",
    "    v_sorted = v[:,ev_sort_idx]\n",
    "    scores = X @ v_sorted\n",
    "    return v_sorted, scores, ev_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this work for our sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_trans.T\n",
    "V, T, explained_variance = PCA_algorithm_evd(X)\n",
    "total_variance = np.sum(explained_variance)\n",
    "explained_variance = explained_variance/total_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"2D-example\")\n",
    "#plt.plot(data[0],data[1],'o')\n",
    "plt.plot(data_trans[0],data_trans[1],'o')\n",
    "plt.quiver(0, 0, V[0,0], V[1,0],  angles='xy', scale_units='xy', scale=1, zorder=1e6)\n",
    "plt.quiver(0, 0, V[0,1], V[1,1],  angles='xy', scale_units='xy', scale=1, zorder=1e6)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"2D-loading\")\n",
    "l1 = explained_variance[0]\n",
    "l2 = explained_variance[0]\n",
    "#plt.plot(data[0],data[1],'o')\n",
    "plt.plot(T[:,0],T[:,1],'o')\n",
    "plt.xlabel(\"$t_1$\")\n",
    "plt.ylabel(\"$t_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"2D-check\")\n",
    "plt.plot(data_trans[0],data_trans[1],'o')\n",
    "plt.plot(T[:,0], T[:,1],'o')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay, let's do this for something a bit more _realistic_\n",
    "\n",
    "Motivation, for reduced order modelling of fractional flow reserve (FFR), we would like to make use of the full geometrical data available, but if we include all features we have way more input features than samples. Thus it will be ill posed to quantify the contribution of each feature. PCA can help reduce the full set of features to a lower dimensional set of features such that thevariation of the data is represented as input, but one doesn't need to know the contribution of each particular input.\n",
    "\n",
    "Here we start with a set of synthetic axi-symmetric geometries. Let's do PCA to see what we can find out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data = []\n",
    "num_samples_per_cls = 5\n",
    "offset = 1.5\n",
    "r0 = np.ones(10)\n",
    "x = np.linspace(0,1,10)\n",
    "np.random.seed()\n",
    "plt.figure()\n",
    "for k in range(num_samples_per_cls):\n",
    "    r1 = np.cos(2*np.pi*k*x)  + offset\n",
    "    r_data.append(r1)\n",
    "    plt.plot(x, r1)\n",
    "    \n",
    "plt.figure()\n",
    "for k in range(num_samples_per_cls):\n",
    "    r1 = -1 + 4*np.random.rand()*abs(x - 0.3 - 0.5*np.random.rand())  + offset\n",
    "    r_data.append(r1)\n",
    "    plt.plot(x, r1)\n",
    "\n",
    "plt.figure()\n",
    "for k in range(num_samples_per_cls):\n",
    "    r1 = 1 - 2*np.random.rand()*(x >0.25*np.random.rand())*( x < 0.75) + offset\n",
    "    r_data.append(r1)\n",
    "    plt.plot(x, r1) \n",
    "    \n",
    "r_data = np.array(r_data) + 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try PCA on this data set. Try the following exercises.\n",
    "\n",
    "* Compute the PCA scores for this data.\n",
    "\n",
    "* Can you determine how many components are needed to account for most of the variability (80%, 90%, etc?) in the data?\n",
    "\n",
    "* For a given sample/observation set how well do 1, 2, 3, ... principal components approximate the full feature set?\n",
    "\n",
    "* For a given sample observation what PC is it most like? \n",
    "\n",
    "* How well does this single PC approximate the observation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_algorithm_evd?\n",
    "# Start implementing your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested solution follows\n",
    "\n",
    "Please\n",
    "\n",
    "Try\n",
    "\n",
    "This\n",
    "\n",
    "At\n",
    "\n",
    "Home\n",
    "\n",
    "before \n",
    "\n",
    "scrolling\n",
    "\n",
    "further.\n",
    "\n",
    "[There's a monster at the end of this notebook](https://www.goodreads.com/book/show/44186.The_Monster_at_the_End_of_this_Book). You don't want to get to the monster do you?\n",
    "![Image of monster](https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1388193494l/44186.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_data\n",
    "X_all = r_data - np.mean(r_data, axis=0) #PCA assumes 0-mean data\n",
    "# Compute PCA representation\n",
    "train_idx = np.ones(len(X_all))>0\n",
    "X = X_all[train_idx]\n",
    "v_sorted,  scores, eigenvals = PCA_algorithm_evd(X)\n",
    "total_var = np.sum(eigenvals)\n",
    "var_exp = eigenvals/total_var\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(var_exp)\n",
    "plt.ylabel(\"Var explained by PC_i\")\n",
    "plt.xlabel(\"PC index\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(var_exp))\n",
    "plt.ylabel(\"Cumulative variance explained by all PC_0 .. PC_i\")\n",
    "plt.xlabel(\"PC index\")\n",
    "\n",
    "d_idx = 3 # data observation to look at\n",
    "k = 5 # number of PC to include in reconstruction\n",
    "plt.figure()\n",
    "pc = np.argmax(scores[d_idx])\n",
    "plt.plot(r_data[d_idx] -  np.mean(r_data, axis=0), color=\"black\", label='data') # shift data to be centered for comparison with PCs\n",
    "plt.plot(v_sorted[:,0:k] @ scores[d_idx,0:k], label=\"k-PC approx\") # reconstruct with all k PCs\n",
    "plt.plot(scores[d_idx][pc]*v_sorted[:,pc], '--', label='Best PC') # use only the closest PC\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check all cases\n",
    "for d_idx, d in enumerate(r_data):\n",
    "    plt.figure()\n",
    "    pc = np.argmax(scores[d_idx])\n",
    "    plt.plot(r_data[d_idx] -  np.mean(r_data, axis=0), color=\"black\", label='data') # shift data to be centered for comparison with PCs\n",
    "    plt.plot(v_sorted[:,0:k] @ scores[d_idx,0:k], label=\"k-PC approx\") # reconstruct with all k PCs\n",
    "    plt.plot(scores[d_idx][pc]*v_sorted[:,pc], '--', label='Best PC') # use only the closest PC\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further topics\n",
    "\n",
    "* PCA Regression regress onto PC instead of the data itself then compute regression coefficients of X from those of T to avoid overfitting issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA regression with NHANES data\n",
    "\n",
    "Below you find some code to load the [NHANES](https://wwwn.cdc.gov/nchs/nhanes) 2015-2016 data into a [pandas](https://pandas.pydata.org/pandas-docs/stable/?v=20191001113306) data frame. Subsequently we will compare linear regression of blood pressure onto other variables in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmx = pd.read_sas(\"BMX_I.XPT\")\n",
    "df_bpx = pd.read_sas(\"BPX_I.XPT\")\n",
    "df = df_bmx.join(df_bpx, lsuffix=\"BMX\", rsuffix=\"BPX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHANES\n",
    "Here is an overview of the variable names\n",
    "https://wwwn.cdc.gov/nchs/nhanes/Search/variablelist.aspx?Component=Examination&CycleBeginYear=2015\n",
    "\n",
    "We will use only a few of these and limit ourselves to the continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_vars = \"\"\"BPXDI2\n",
    "BPXPLS\n",
    "BPXSY2\"\"\".split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_vars = \"\"\"BPXCHR\n",
    "BMDAVSAD\n",
    "BMIWT\n",
    "BMXARMC\n",
    "BMXARML\n",
    "BMXBMI\n",
    "BMXHT\n",
    "BMXLEG\n",
    "BMXRECUM\n",
    "BMXSAD1\n",
    "BMXSAD2\n",
    "BMXSAD3\n",
    "BMXSAD4\n",
    "BMXWAIST\n",
    "BMXWT\"\"\".split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.median(), inplace=True)\n",
    "df_combined = df[response_vars+predictor_vars]\n",
    "df_responses = df[response_vars]\n",
    "df_predictors = df[predictor_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data frame to raw data\n",
    "The code that follows makes use of the `sklearn` and `statsmodels` integration with `pandas` dataframes for ease of use. Thus instead of the PCA code presented above we use `sklearn` to compute the PCA of the data.\n",
    "To work directly on the data as a matrix, we can access `df.values` which is a numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictors.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_combined.plot(x=\"BMXBMI\", y=\"BPXPLS\", kind=\"scatter\", alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sm.OLS(df_responses[\"BPXPLS\"], sm.add_constant(df_predictors, prepend=False)).fit()\n",
    "print('OLS on original data')\n",
    "print(res.params)\n",
    "print(res.aic)\n",
    "print(res.rsquared)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can try the same with PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "pca_obj = sklearn.decomposition.PCA()\n",
    "pca_obj.fit(df_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_scores = pca_obj.transform(df_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = sm.OLS(df_responses[\"BPXPLS\"], sm.add_constant(PCA_scores, prepend=False)).fit()\n",
    "print('OLS on PCA data')\n",
    "print(res.params)\n",
    "print(res.aic)\n",
    "print(res.rsquared)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(pca_obj.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
