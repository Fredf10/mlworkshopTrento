{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines for classification\n",
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines may be applied for both regression and classifiaction, but here we will focus on the latter.\n",
    "\n",
    "For the simplest example, take a two-imensional plane, spanned by two features of a concept you wish to study. In this plane we have a few sample points which, We wish to now find a linear curve which dissects the data populating this plane into two categories. If this is possible the set is linearly separable, and for the rest of the discussion let us assume our set is so. If it is not, a trick we will introduce at a later point which introduces a function called a Kernel, will transform the input features into more dimensions and if the set is linearly sepearable in this new space, the hyperplane identified to perform this task can be projected onto the 2D plane and yield a boundary. \n",
    "\n",
    "For the simplest case in 2D, we have a hyperplane manifesting as a straight line intersecting the feature plane. The normal of the plane is given by the vector $\\mathbf{w}$, a point in the plane has a position $\\mathbf{x}$ relative to the origin. Points in one class are labeled by +1, and the other is labeled with -1.\n",
    "We now insist that the dot product of these vectors plus some constant $b$ should ble larger than one for a point on the side of the plane belonging to the +1 class, while the sum should be less than -1 for points in the -1 class.  These sets of equations, are describing the situation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b \\geq 1,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} \\cdot \\mathbf{x} + b \\leq -1.\n",
    "\\end{equation}\n",
    "\n",
    "And since the labels $y_i$ is $\\pm 1$ depending on the location of the point in relation to the dividing line we can simplify both contstraints to:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i (\\mathbf{x}_i \\cdot \\mathbf{w} + b) - 1 = 0.\n",
    "\\end{equation}\n",
    "\n",
    "We can then use the difference between two support vectors defining each edge of the margin and alligning this with the plane normal \\bf{w}.\n",
    "\n",
    "\\begin{equation}\n",
    "width = (\\mathbf{x}_+ - \\mathbf{x}_-) \\cdot \\frac{\\mathbf{w}}{||\\mathbf{w}||},\n",
    "\\end{equation}\n",
    "\n",
    "and by some algebra we can find that to maximize the margin we can minimize\n",
    "\n",
    "\\begin{equation}\n",
    "min \\frac{1}{2} ||\\mathbf{w}||^2\n",
    "\\end{equation}\n",
    "\n",
    "to find the maximum boundary margin. We introduce the Lagrangian function where all constrainta are added to the optimization function by lagrangian multipliers $\\alpha_i$, which are zero for most of the constraints with $n$ being the number of calssified data points. The constraints which are not support vectors at the margin tend to have an alpha close to zero.\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{2} ||\\mathbf{w}||^2 - \\sum_i^n \\alpha_i \\langle (\\mathbf{w} \\cdot \\mathbf{x} + b) - 1 \\rangle,\n",
    "\\end{equation}\n",
    "boundary\n",
    "by restriction to the global minimum we can rewrite this to\n",
    "\\begin{equation}\n",
    "L = \\sum_i^n \\alpha_i - \\frac{1}{2} \\sum_i^n \\sum_j^n \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i \\cdot \\mathbf{x}_j.\n",
    "\\end{equation}\n",
    "\n",
    "This is the restriction to a linear descision boundary and margins. For more complex boundaries one can introduce the kernel as discussed above to replace the linear dotproduct, we will see an example of how this can be done later. \n",
    "\n",
    "\\begin{equation}\n",
    "L = \\sum_i^n \\alpha_i - \\frac{1}{2} \\sum_i^n \\sum_j^n \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j).\n",
    "\\end{equation}\n",
    "\n",
    "Source: https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8\n",
    "\n",
    "The Figure below illustrates the support vecotr machine for the linear two-dimensional case.\n",
    "\n",
    "<img src=\"http://folk.ntnu.no/nikolalb/ML_ws/svm_slack.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2 element Windkessel model incorporates the compliant and resistive properties of the systemic arteries to compute the blood pressure response to an imposed blood flow from the heart. \n",
    "\n",
    "Varying the parameters for arterial compliance and resistance and creating 8000 Windkessel configurations and computed blood pressures we can label them as hypertensive and normotensive cases. The computed pressures are aortic pressures, but they were still classified using the European Society of Cardiology guidelines of systolic brachial pressure over 140 mmHg and diastolic pressure over 90 mmHg for hypertension. No transfer function between brachia and aorta was applied. The resulting domain where the input parameters compliance and resistance ($C$ and $R$) were plotted with their resulting systolic pressures is shown below. Orange dots indicate an hypertensive state, while people outside this range is classified as normotensive with blue dots, even if they are highly isolated systolic hypertensive. This adds some complexity to the issue since it is now not only a singe cut off value.\n",
    "\n",
    "<img src=\"http://folk.ntnu.no/nikolalb/ML_ws/Hypertensives_Psys.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we classify hypertensives directly from measured parameter sets? \n",
    "\n",
    "Let us try making this work using only tensorflow and the assumption that the set is linearly separable. From the figure it clearly is not, but let us try either way.\n",
    "\n",
    "Code source: https://www.youtube.com/watch?v=zErT-VtYOHk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # depreciate warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# load data and split training set into actual training and validation sets\n",
    "#===================================================\n",
    "dataNameTrain = \"WK2DataLabeled_train\" # for training the model\n",
    "dataNameTest = \"WK2DataLabeled_test\" # for testing the model\n",
    "\n",
    "testFracTest = 0.9\n",
    "colNames = [\"C\", \"R\", \"P_sys\", \"P_dia\", \"PP\", \"Hyp\"]\n",
    "featureCols = [0, 1] # C and R\n",
    "labelCol = 5 # Label\n",
    "\n",
    "data = np.genfromtxt(dataNameTrain, dtype=np.float32,skip_header=True)\n",
    "x, y = data[:,featureCols].copy(), data[:,labelCol].copy()\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(x, y, test_size=testFracTest, random_state=35)\n",
    "#train_Y = np.array(train_Y)\n",
    "\n",
    "train_Y= train_Y[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "val_Y= val_Y[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "print(type(train_X), type(train_Y))\n",
    "\n",
    "hyp_C = [d[0] for i,d in enumerate(train_X) if train_Y[i]==1]\n",
    "hyp_R = [d[1] for i,d in enumerate(train_X) if train_Y[i]==1]\n",
    "norm_C = [d[0] for i,d in enumerate(train_X) if train_Y[i]==-1]\n",
    "norm_R = [d[1] for i,d in enumerate(train_X) if train_Y[i]==-1]\n",
    "        \n",
    "#======================================================\n",
    "# Visualize initial points\n",
    "#======================================================\n",
    "plt.plot(hyp_C, hyp_R,'o',label='Hypertensive')\n",
    "plt.plot(norm_C, norm_R,'x',label='Normotensive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# scale inputs\n",
    "#===================================================\n",
    "#scaler = preprocessing.StandardScaler()\n",
    "#train_X = scaler.fit_transform(train_X) # fit (find mu and std) scale and transform data\n",
    "#val_X  = scaler.transform(val_X) # transform data based on mu and std from training/learning set\n",
    "#===================================================\n",
    "# set parameters for SVM\n",
    "#===================================================\n",
    "batch_size = 60\n",
    "epochs = 5000\n",
    "trainingRate=0.01\n",
    "\n",
    "#==============================\n",
    "# Create placeholders and linear parameter placeholders for defining a hyperplane line.\n",
    "#==============================\n",
    "sess = tf.Session()\n",
    "\n",
    "x_data = tf.placeholder(shape=[None,2], dtype=tf.float32,name=\"RCinput\")\n",
    "y_target = tf.placeholder(shape=[None,1], dtype=tf.float32,name=\"Label\")\n",
    "\n",
    "A = tf.Variable(tf.random_normal(shape=[2,1])) #Containing vector w, components\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1])) #Boundary bias, b\n",
    "\n",
    "model_output = tf.subtract(tf.matmul(x_data, A), b) # w . x - b \n",
    "\n",
    "#==============================\n",
    "# Maximum magin loss function\n",
    "#==============================\n",
    "l2_norm = tf.reduce_sum(tf.square(A))\n",
    "alpha = tf.constant([0.01]) # characterizing the \"Hardness\" of the decision boundary. Lower alpha allows more points to cross.\n",
    "classification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1.0,tf.multiply(model_output,y_target))))\n",
    "# 1 - y_i (w . x_i +b) >= 0\n",
    "loss = tf.add(tf.multiply(alpha,l2_norm), classification_term)\n",
    "# alpha * ||w||^2 + max(1 - y_i (w . x_i +b), 0 )\n",
    "\n",
    "#==============================\n",
    "# Prediction and accuracy\n",
    "#==============================\n",
    "\n",
    "prediction = tf.sign(model_output)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target),tf.float32))\n",
    "residuals = prediction - y_target\n",
    "\n",
    "#=========================================================\n",
    "# Set optimization method and initialize model variables\n",
    "#=========================================================\n",
    "opt_ref = tf.train.GradientDescentOptimizer(trainingRate)\n",
    "train_step = opt_ref.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "# Run session for number of epochs\n",
    "#=========================================================\n",
    "\n",
    "loss_vec = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    rand_index = np.random.choice(len(train_X), size=batch_size)\n",
    "    rand_x = train_X[rand_index,:]\n",
    "    rand_y = train_Y[rand_index,:]\n",
    "    \n",
    "    _, train_loss = sess.run([train_step, loss],\n",
    "                                      feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    test_loss, test_resids = sess.run([loss,residuals],\n",
    "                                     feed_dict={x_data: val_X, y_target: val_Y})\n",
    "    loss_vec.append(train_loss)\n",
    "    train_acc_temp = sess.run(accuracy, feed_dict={x_data: train_X, y_target: train_Y})\n",
    "    train_accuracy.append(train_acc_temp)\n",
    "    test_acc_temp = sess.run(accuracy, feed_dict={x_data: val_X, y_target: val_Y})\n",
    "    test_accuracy.append(test_acc_temp)\n",
    "    \n",
    "    if (i+1)%500==0:\n",
    "        print('Step #'+str(i+1)+' A = ' + str(sess.run(A)) + ' b = ' + str(sess.run(b)))\n",
    "        print('Loss = ' + str(train_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================\n",
    "# Extract coefficients of hyperplane and classify\n",
    "#======================================================\n",
    "[[a1],[a2]] = sess.run(A)\n",
    "[[b_out]] = sess.run(b)\n",
    "print(a1,a2,b_out)\n",
    "slope = -a1/a2\n",
    "y_intercept = b_out/a2\n",
    "x1_values = [d[0] for d in val_X]\n",
    "best_fit = []\n",
    "for i in x1_values:\n",
    "    best_fit.append(slope*i+y_intercept)\n",
    "\n",
    "hyp_C = [d[0] for i,d in enumerate(val_X) if val_Y[i]==1]\n",
    "hyp_R = [d[1] for i,d in enumerate(val_X) if val_Y[i]==1]\n",
    "norm_C = [d[0] for i,d in enumerate(val_X) if val_Y[i]==-1]\n",
    "norm_R = [d[1] for i,d in enumerate(val_X) if val_Y[i]==-1]\n",
    "        \n",
    "#======================================================\n",
    "# Visualize results\n",
    "#======================================================\n",
    "plt.plot(hyp_C, hyp_R,'o',label='Hypertensive')\n",
    "plt.plot(norm_C, norm_R,'x',label='Normotensive')\n",
    "plt.plot(x1_values, best_fit, 'r-', label='Linear Separator',linewidth=3)\n",
    "#plt.plot(x1_values, 4.8 + 1./slope*np.array(x1_values),label='mirror')\n",
    "#plt.ylim([-0.5,4.5])\n",
    "plt.legend()\n",
    "plt.title('C and R with hyperplane - training data')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('R')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracy, 'k--', label='Training Accuracy')\n",
    "plt.plot(test_accuracy, 'r--', label='Test Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Train and test set accuracies')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "plt.plot(loss_vec,'k-')\n",
    "plt.title('Loss per generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic kernel\n",
    "\n",
    "Let us now try adding a kernel, in stead of a plain dot product between two-dimensional vectors let us now try using the square of a dot product, which should allow us to employ some more complex polynomial functions and other support vectors for determining our decision boundary. \n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{2}||\\mathbf{w}||^2 + \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j (\\mathbf{x}_i \\cdot \\mathbf{x}_j )\n",
    "\\end{equation}\n",
    "\n",
    "We substitute the dot-ptoduct for the kernel $K(\\mathbf{x},\\mathbf{x'})$\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{2}||\\mathbf{w}||^2 + \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x_i},\\mathbf{x_j})\n",
    "\\end{equation}\n",
    "\n",
    "Here will apply a a second order polynomial kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K(\\mathbf{x},\\mathbf{x'}) = (1 + \\mathbf{x}^T \\cdot \\mathbf{x'})^2\n",
    "\\end{equation}\n",
    "\n",
    "Problem 1:\n",
    "- Unfortunately I was myself unable to correctly implement this example in time. Can you implement it? Alterniatively skip ahead to the gaussian kernel problem to learn more and return to this problem later. The loss function and prediction functions must be properly implemented using the new kernel. \n",
    "\n",
    "For the original source, and other ideas/examples go to: https://github.com/nfmcclure/tensorflow_cookbook/tree/master/04_Support_Vector_Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# load data and split training set into actual training and validation sets\n",
    "#===================================================\n",
    "dataNameTrain = \"WK2DataLabeled_train\" # for training the model\n",
    "dataNameTest = \"WK2DataLabeled_test\" # for testing the model\n",
    "\n",
    "testFracTest = 0.97\n",
    "colNames = [\"C\", \"R\", \"P_sys\", \"P_dia\", \"PP\", \"Hyp\"]\n",
    "featureCols = [0, 1] # C and R\n",
    "labelCol = 5 # Label\n",
    "\n",
    "data = np.genfromtxt(dataNameTrain, dtype=np.float32,skip_header=True)\n",
    "x, y = data[:,featureCols].copy(), data[:,labelCol].copy()\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(x, y, test_size=testFracTest, random_state=35)\n",
    "#train_Y = np.array(train_Y)\n",
    "\n",
    "train_Y= train_Y[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "val_Y= val_Y[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "\n",
    "print(train_X.shape, train_Y.shape)\n",
    "print(type(train_X), type(train_Y))\n",
    "\n",
    "hyp_C = [d[0] for i,d in enumerate(train_X) if train_Y[i]==1]\n",
    "hyp_R = [d[1] for i,d in enumerate(train_X) if train_Y[i]==1]\n",
    "norm_C = [d[0] for i,d in enumerate(train_X) if train_Y[i]==-1]\n",
    "norm_R = [d[1] for i,d in enumerate(train_X) if train_Y[i]==-1]\n",
    "        \n",
    "#======================================================\n",
    "# Visualize training points\n",
    "#======================================================\n",
    "plt.plot(hyp_C, hyp_R,'o',label='Hypertensive')\n",
    "plt.plot(norm_C, norm_R,'x',label='Normotensive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# scale inputs, optional\n",
    "#===================================================\n",
    "#scaler = preprocessing.StandardScaler()\n",
    "#train_X = scaler.fit_transform(train_X) # fit (find mu and std) scale and transform data\n",
    "#val_X  = scaler.transform(val_X) # transform data based on mu and std from training/learning set\n",
    "\n",
    "#===================================================\n",
    "# set parameters for SVM\n",
    "#===================================================\n",
    "batch_size = train_X.shape[0]\n",
    "epochs = 10000\n",
    "trainingRate=0.01\n",
    "\n",
    "#==============================\n",
    "# Create placeholders\n",
    "#==============================\n",
    "sess = tf.Session()\n",
    "\n",
    "x_data = tf.placeholder(shape=[None,2], dtype=tf.float32,name=\"RCinput\")\n",
    "y_target = tf.placeholder(shape=[None,1], dtype=tf.float32,name=\"Label\")\n",
    "prediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "\n",
    "b = tf.Variable(tf.random_normal(shape=[1,batch_size])) \n",
    "# b is here a vector containing the lagrangian multipliers (alphas as opposed to the bias) for the hyperplane\n",
    "\n",
    "#==================================================\n",
    "# Set kernel\n",
    "#==================================================\n",
    "# Quadratic\n",
    "dotprod = tf.matmul(x_data,tf.transpose(x_data))\n",
    "quad_kernel = tf.matmul(tf.add(1.,dotprod), tf.add(1.,dotprod))\n",
    "\n",
    "#=================================# \n",
    "# Loss function for minimization  #\n",
    "#=================================#\n",
    "#...\n",
    "\n",
    "#==============================\n",
    "# Prediction computation\n",
    "#==============================\n",
    "#rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])\n",
    "#rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])\n",
    "#pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data,tf.transpose(prediction_grid)))), tf.transpose(rB))\n",
    "#pred_kernel = tf.square(tf.matmul(pred_sq_dist,tf.transpose(pred_sq_dist)))\n",
    "#prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b),pred_kernel)\n",
    "#prediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output))\n",
    "#accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction),tf.squeeze(y_target)), tf.float32))\n",
    "\n",
    "#print(type(prediction_output), type(prediction))\n",
    "#print(prediction_output.shape, prediction.shape)\n",
    "\n",
    "#=========================================================\n",
    "# Set optimization method and initialize model variables\n",
    "#=========================================================\n",
    "opt_ref = tf.train.GradientDescentOptimizer(trainingRate)\n",
    "train_step = opt_ref.minimize(loss)\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# Visualize initial condition\n",
    "# Create a mesh to plot points in\n",
    "x_min, x_max = train_X[:, 0].min() - 1, train_X[:, 0].max() + 1\n",
    "y_min, y_max = train_X[:, 1].min() - 1, train_X[:, 1].max() + 1\n",
    "x1, x2 = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "grid_points = np.c_[x1.ravel(), x2.ravel()]\n",
    "\n",
    "[pred_init] = sess.run(prediction, feed_dict={x_data: train_X,\n",
    "                                                   y_target: train_Y,\n",
    "                                                   prediction_grid: grid_points})\n",
    "print(pred_init.shape, grid_points.shape)\n",
    "grid_predictions = pred_init.reshape(x1.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.contouf(x1,x2,grid_predictions)\n",
    "plt.plot(hyp_C, hyp_R,'o',label='Hypertensive')\n",
    "plt.plot(norm_C, norm_R,'x',label='Normotensive')\n",
    "plt.plot(pred_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "# Run session for number of epochs\n",
    "#=========================================================\n",
    "\n",
    "loss_vec = []\n",
    "train_accuracy = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    rand_index = np.random.choice(len(train_X), size=batch_size)\n",
    "    X = train_X[rand_index,:]\n",
    "    Y = train_Y[rand_index,:]\n",
    "    sess.run(train_step, feed_dict={x_data: X, y_target: Y})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: X, y_target: Y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    acc_temp = sess.run(accuracy, feed_dict={x_data: X,y_target: Y,prediction_grid:X})\n",
    "    train_accuracy.append(acc_temp)\n",
    "    \n",
    "    if (i+1)%1000==0:\n",
    "        print('Epoch #'+str(i+1))\n",
    "        print('Loss = ' + str(temp_loss))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh to plot points in\n",
    "x_min, x_max = train_X[:, 0].min() - 1, train_X[:, 0].max() + 1\n",
    "y_min, y_max = train_X[:, 1].min() - 1, train_X[:, 1].max() + 1\n",
    "x1, x2 = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "grid_points = np.c_[x1.ravel(), x2.ravel()]\n",
    "#print(x1.ravel(), x1.ravel().shape)\n",
    "#print(type(grid_points), grid_points.shape)\n",
    "[grid_predictions] = sess.run(prediction, feed_dict={x_data: train_X,\n",
    "                                                   y_target: train_Y,\n",
    "                                                   prediction_grid: grid_points})\n",
    "grid_predictions = grid_predictions.reshape(x1.shape)\n",
    "#print(grid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Create a mesh to plot points in\n",
    "x_min, x_max = train_X[:, 0].min() - 1, train_X[:, 0].max() + 1\n",
    "y_min, y_max = train_X[:, 1].min() - 1, train_X[:, 1].max() + 1\n",
    "x1, x2 = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "grid_points = np.c_[x1.ravel(), x2.ravel()]\n",
    "#print(x1.ravel(), x1.ravel().shape)\n",
    "#print(type(grid_points), grid_points.shape)\n",
    "[grid_predictions] = sess.run(prediction, feed_dict={x_data: train_X,\n",
    "                                                   y_target: train_Y,\n",
    "                                                   prediction_grid: grid_points})\n",
    "grid_predictions = grid_predictions.reshape(x1.shape)\n",
    "#print(grid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================\n",
    "# Plot points and grid\n",
    "#==================================\n",
    "plt.contourf(x1, x2, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.plot(hyp_C, hyp_R, 'bo', label='Hypertensive')\n",
    "plt.plot(norm_C, norm_R, 'kx', label='Normotensive')\n",
    "plt.title('Gaussian SVM Results for training points')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('R')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([-0.0, 3.5])\n",
    "plt.xlim([-0.0, 7.5])\n",
    "plt.show()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.plot(train_accuracy, 'k-', label='Accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(loss_vec, 'k-')\n",
    "plt.title('Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "#========================================\n",
    "# Plot validation points\n",
    "#========================================\n",
    "\n",
    "hyp_C_val = [d[0] for i,d in enumerate(val_X) if val_Y[i]==1]\n",
    "hyp_R_val = [d[1] for i,d in enumerate(val_X) if val_Y[i]==1]\n",
    "norm_C_val = [d[0] for i,d in enumerate(val_X) if val_Y[i]==-1]\n",
    "norm_R_val = [d[1] for i,d in enumerate(val_X) if val_Y[i]==-1]\n",
    "\n",
    "plt.contourf(x1, x2, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.plot(hyp_C_val, hyp_R_val, 'bo', label='Hypertensive')\n",
    "plt.plot(norm_C_val, norm_R_val, 'kx', label='Normotensive')\n",
    "plt.title('Gaussian SVM Results for validation points')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('R')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([-0.0, 3.5])\n",
    "plt.xlim([-0.0, 7.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian kernel\n",
    "\n",
    "now we attempt with a kernel on the form\n",
    "\n",
    "\\begin{equation}\n",
    "K(\\mathbf{x},\\mathbf{x}') = e^{\\gamma |\\mathbf{x} - \\mathbf{x}'|^2}, \\gamma < 0\n",
    "\\end{equation}\n",
    "\n",
    "For the original source, and other ideas/examples go to: https://github.com/nfmcclure/tensorflow_cookbook/tree/master/04_Support_Vector_Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# load data and split training set into actual training and validation sets\n",
    "#===================================================\n",
    "dataNameTrain = \"WK2DataLabeled_train\" # for training the model\n",
    "dataNameTest = \"WK2DataLabeled_test\" # for testing the model\n",
    "\n",
    "testFracTest = 0.96\n",
    "colNames = [\"C\", \"R\", \"P_sys\", \"P_dia\", \"PP\", \"Hyp\"]\n",
    "featureCols = [0, 1] # C and R\n",
    "labelCol = 5 # Label\n",
    "\n",
    "data = np.genfromtxt(dataNameTrain, dtype=np.float32,skip_header=True)\n",
    "x, y = data[:,featureCols].copy(), data[:,labelCol].copy()\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(x, y, test_size=testFracTest, random_state=35)\n",
    "#train_Y = np.array(train_Y)\n",
    "\n",
    "train_Y= 1.*train_Y[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "val_Y= val_Y[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "\n",
    "#print(train_X.shape, train_Y.shape)\n",
    "#print(type(train_X), type(train_Y))\n",
    "\n",
    "hyp_C = [d[0] for i,d in enumerate(train_X) if train_Y[i]==1]\n",
    "hyp_R = [d[1] for i,d in enumerate(train_X) if train_Y[i]==1]\n",
    "norm_C = [d[0] for i,d in enumerate(train_X) if train_Y[i]==-1]\n",
    "norm_R = [d[1] for i,d in enumerate(train_X) if train_Y[i]==-1]\n",
    "        \n",
    "#======================================================\n",
    "# Visualize training points\n",
    "#======================================================\n",
    "plt.plot(hyp_C, hyp_R,'o',label='Hypertensive')\n",
    "plt.plot(norm_C, norm_R,'x',label='Normotensive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# scale inputs\n",
    "#===================================================\n",
    "#scaler = preprocessing.StandardScaler()\n",
    "#train_X = scaler.fit_transform(train_X) # fit (find mu and std) scale and transform data\n",
    "#val_X  = scaler.transform(val_X) # transform data based on mu and std from training/learning set\n",
    "#===================================================\n",
    "# set parameters for SVM\n",
    "#===================================================\n",
    "batch_size = train_X.shape[0]\n",
    "epochs = 1000\n",
    "trainingRate=0.01\n",
    "gamma_const = -100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#==============================\n",
    "# Create placeholders and linear parameter placeholders for defining a hyperplane line.\n",
    "#==============================\n",
    "sess = tf.Session()\n",
    "\n",
    "x_data = tf.placeholder(shape=[None,2], dtype=tf.float32,name=\"RCinput\")\n",
    "y_target = tf.placeholder(shape=[None,1], dtype=tf.float32,name=\"Label\")\n",
    "prediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "\n",
    "b = tf.Variable(tf.random_normal(shape=[1,batch_size])) # Not linear bias b, but vector of lagrangian multipliers alpha\n",
    "#==================================================\n",
    "# Set kernel\n",
    "#==================================================\n",
    "# Gaussian\n",
    "gamma = tf.constant(gamma_const)\n",
    "dist = tf.reduce_sum(tf.square(x_data), 1) # sum (x^2) ---> vector of N_batch elements R^2 + C^2\n",
    "dist = tf.reshape(dist, [-1,1]) #Reshape to 1D vector\n",
    "sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(x_data,tf.transpose(x_data)))), tf.transpose(dist))\n",
    "# dist^T + (dist - 2* x cross x^T)\n",
    "gaussian_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists))) # exp(gamma * |(x^2)^T + x^2 - 2 * x^2_matrix|)\n",
    "\n",
    "#==============================\n",
    "# Specify loss function\n",
    "#==============================\n",
    "first_term = tf.reduce_sum(b) # sum alpha\n",
    "b_vec_cross = tf.matmul(tf.transpose(b), b) # alpha^T x alpha (row vectors)\n",
    "y_target_cross = tf.matmul(y_target, tf.transpose(y_target)) # y x y^T (column vectors)\n",
    "second_term = tf.reduce_sum(tf.multiply(gaussian_kernel, tf.multiply(b_vec_cross,y_target_cross))) \n",
    "# sum (exp(...) * bxb * yxy)\n",
    "loss = tf.negative(tf.subtract(first_term, second_term)) #-(sum b - sum (K bxb yxy))\n",
    "\n",
    "#==============================\n",
    "# Prediction function set up\n",
    "#==============================\n",
    "rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1]) # sum R^2+C^2, 1D vector\n",
    "rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1]) #Square distances to points in grid\n",
    "pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data,tf.transpose(prediction_grid)))), tf.transpose(rB))\n",
    "# Squared distances between grid points and data points\n",
    "pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist))) # Evaluate kernel\n",
    "prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b),pred_kernel) # (y_vec^T * b) * K\n",
    "prediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output)) #sign of deviation from mean\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction),tf.squeeze(y_target)), tf.float32)) \n",
    "# sum correct predictions\n",
    "\n",
    "#print(type(prediction_output), type(prediction))\n",
    "#print(prediction_output.shape, prediction.shape)\n",
    "\n",
    "#=========================================================\n",
    "# Set optimization method and initialize model variables\n",
    "#=========================================================\n",
    "opt_ref = tf.train.GradientDescentOptimizer(trainingRate)\n",
    "train_step = opt_ref.minimize(loss)\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "# Run session for number of epochs\n",
    "#=========================================================\n",
    "\n",
    "loss_vec = []\n",
    "train_accuracy = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    rand_index = np.random.choice(len(train_X), size=batch_size)\n",
    "    X = train_X[rand_index,:]\n",
    "    Y = train_Y[rand_index,:]\n",
    "    sess.run(train_step, feed_dict={x_data: X, y_target: Y})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: X, y_target: Y})\n",
    "    loss_vec.append(temp_loss)\n",
    "    acc_temp = sess.run(accuracy, feed_dict={x_data: X,y_target: Y,prediction_grid:X})\n",
    "    train_accuracy.append(acc_temp)\n",
    "    \n",
    "    if (i+1)%1000==0:\n",
    "        print('Epoch #'+str(i+1))\n",
    "        print('Loss = ' + str(temp_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#====================================\n",
    "# Create a mesh to plot points in\n",
    "#====================================\n",
    "x_min, x_max = train_X[:, 0].min() - 1, train_X[:, 0].max() + 1\n",
    "y_min, y_max = train_X[:, 1].min() - 1, train_X[:, 1].max() + 1\n",
    "x1, x2 = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "grid_points = np.c_[x1.ravel(), x2.ravel()] #Turn grid into lists and join pairwise to a [.., 2] matrix\n",
    "\n",
    "[grid_predictions] = sess.run(prediction, feed_dict={x_data: train_X,\n",
    "                                                   y_target: train_Y,\n",
    "                                                   prediction_grid: grid_points}) # Evaluate training for gridpoints\n",
    "grid_predictions = grid_predictions.reshape(x1.shape) # Reshape to grid for plotting.\n",
    "#print(grid_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#==================================\n",
    "# Plot points and grid\n",
    "#==================================\n",
    "plt.contourf(x1, x2, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.plot(hyp_C, hyp_R, 'bo', label='Hypertensive')\n",
    "plt.plot(norm_C, norm_R, 'kx', label='Normotensive')\n",
    "plt.title('Gaussian SVM Results for training points')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('R')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([-0.0, 3.5])\n",
    "plt.xlim([-0.0, 7.5])\n",
    "plt.show()\n",
    "\n",
    "#============================\n",
    "# Plot training accuracy\n",
    "#============================\n",
    "plt.plot(train_accuracy, 'k-', label='Accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#======================\n",
    "# Plot loss over time\n",
    "#======================\n",
    "plt.plot(loss_vec, 'k-')\n",
    "plt.title('Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "#========================================\n",
    "# Plot validation points\n",
    "#========================================\n",
    "x_min, x_max = val_X[:, 0].min() - 1, val_X[:, 0].max() + 1\n",
    "y_min, y_max = val_X[:, 1].min() - 1, val_X[:, 1].max() + 1\n",
    "x1, x2 = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "grid_points = np.c_[x1.ravel(), x2.ravel()]\n",
    "print(x1.ravel(), x1.ravel().shape)\n",
    "print(type(grid_points), grid_points.shape)\n",
    "[grid_predictions] = sess.run(prediction, feed_dict={x_data: val_X,\n",
    "                                                   y_target: val_Y,\n",
    "                                                   prediction_grid: grid_points})\n",
    "grid_predictions = grid_predictions.reshape(x1.shape)\n",
    "\n",
    "hyp_C_val = [d[0] for i,d in enumerate(val_X) if val_Y[i]==1]\n",
    "hyp_R_val = [d[1] for i,d in enumerate(val_X) if val_Y[i]==1]\n",
    "norm_C_val = [d[0] for i,d in enumerate(val_X) if val_Y[i]==-1]\n",
    "norm_R_val = [d[1] for i,d in enumerate(val_X) if val_Y[i]==-1]\n",
    "\n",
    "plt.contourf(x1, x2, grid_predictions, cmap=plt.cm.Paired, alpha=0.8)\n",
    "#plt.plot(hyp_C_val, hyp_R_val, 'bo', label='Hypertensive')\n",
    "#plt.plot(norm_C_val, norm_R_val, 'kx', label='Normotensive')\n",
    "plt.title('Gaussian SVM Results for validation points')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('R')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([-0.0, 3.5])\n",
    "plt.xlim([-0.0, 7.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#======================================================\n",
    "# Extract coefficients of b-vector\n",
    "#======================================================\n",
    "b_out = sess.run(b)\n",
    "#print(sess.run(prediction_output))\n",
    "print(b_out)\n",
    "\n",
    "#dist = np.sum(train_X**2,1)\n",
    "#sq_dist = dist - 2*np.matmul(train_X,np.transpose(train_X))+ np.transpose(dist)\n",
    "#gaussian = np.exp(gamma_const*sq_dist)\n",
    "#gaussian = np.exp(gamma_const*(train_X)**2)\n",
    "#output = np.matmul(b_out,gaussian)\n",
    "#print('Output',output.shape)\n",
    "plt.plot(b_out.flatten(), 'k--')\n",
    "plt.show()\n",
    "  \n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "       \n",
    "ax.scatter(train_X[:,0], train_X[:,1], b_out)\n",
    "ax.set_xlabel('C')\n",
    "ax.set_ylabel('R')\n",
    "ax.set_zlabel('b')\n",
    "plt.show()\n",
    "\n",
    "#pred_out = sess.run(prediction_grid, feed_dict={x_data: train_X})\n",
    "#print(type(pred_out),pred_out)\n",
    "#ouput1 = sess.run(prediction)\n",
    "#print(output1)\n",
    "#[[b_out]] = sess.run(b)\n",
    "#print(a1,a2,b_out)\n",
    "#slope = -a1/a2\n",
    "#y_intercept = b_out/a2\n",
    "#x1_values = [d[0] for d in val_X]\n",
    "#best_fit = []\n",
    "#for i in x1_values:\n",
    "#    best_fit.append(slope*i+y_intercept)\n",
    "\n",
    "#hyp_C = [d[0] for i,d in enumerate(val_X) if val_Y[i]==1]\n",
    "#hyp_R = [d[1] for i,d in enumerate(val_X) if val_Y[i]==1]\n",
    "#norm_C = [d[0] for i,d in enumerate(val_X) if val_Y[i]==-1]\n",
    "#norm_R = [d[1] for i,d in enumerate(val_X) if val_Y[i]==-1]\n",
    "        \n",
    "#======================================================\n",
    "# Visualize results\n",
    "#======================================================\n",
    "plt.plot(hyp_C, hyp_R,'o',label='Hypertensive')\n",
    "plt.plot(norm_C, norm_R,'x',label='Normotensive')\n",
    "plt.plot(b_out, 'k--')\n",
    "#plt.plot(x1_values, 4.8 + 1./slope*np.array(x1_values),label='mirror')\n",
    "plt.ylim([-0.5,4.5])\n",
    "plt.legend()\n",
    "plt.title('C and R with hyperplane - training data')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('R')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracy, 'k--', label='Training Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training batch accuracies')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "plt.plot(loss_vec,'k-')\n",
    "plt.title('Loss per generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
